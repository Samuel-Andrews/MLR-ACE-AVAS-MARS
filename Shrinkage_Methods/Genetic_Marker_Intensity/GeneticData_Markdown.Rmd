---
title: "Shrinkage-methods"
author: "Samuel Andrews & Mikolaj Wieczorek"
date: "2/28/2020"
output:
  md_document:
    variant: markdown_github
    
---

# Genetic Marker Intensity Measurments 
```{r include=FALSE}
#Libraries
library(tidyr)
library(dplyr)
library(ggplot2)
library(nnet)
library(Ecfun)
library(car)
library(ISLR)
library(MASS)
library(glmnet)
```
```{r include=FALSE}
setwd("~/OneDrive - MNSCU/myGithub/Supervised_Learning/Multiple_Linear_Regression/MachineLearning-SupervisedLearning/Data")
load("~/OneDrive - MNSCU/myGithub/Supervised_Learning/Multiple_Linear_Regression/MachineLearning-SupervisedLearning/Data/mult.Rdata")
load("~/OneDrive - MNSCU/myGithub/Supervised_Learning/Multiple_Linear_Regression/MachineLearning-SupervisedLearning/Data/Regression.Rdata")
```

```{r include=FALSE}
PredAcc = function(y, ypred){
  RMSEP = sqrt(mean((y-ypred)^2))
  MAE = mean(abs(y-ypred))
  MAPE = mean(abs(y-ypred)/y)*100
  cat("RMSEP\n")
  cat("================\n")
  cat(RMSEP, "\n\n")
  cat("MAE\n")
  cat("================\n")
  cat(MAE, "\n\n")
  cat("MAPE\n")
  cat("================\n")
  cat(MAPE, "\n\n")
  return(data.frame(RMSEP = RMSEP, MAE = MAE, MAPE = MAPE))
  
}

myBC = function(y) {
  require(car)
  BCtran(y)
  results = powerTransform(y)
  summary(results)
}

kfold.MLR.log = function(fit,k=10) {
  sum.sqerr = rep(0,k)
  sum.abserr = rep(0,k)
  sum.pererr = rep(0,k)
  y = fit$model[,1]
  y = exp(y)
  x = fit$model[,-1]
  data = fit$model
  n = nrow(data)
  folds = sample(1:k,nrow(data),replace=T)
  for (i in 1:k) {
    fit2 <- lm(formula(fit),data=data[folds!=i,])
    ypred = predict(fit2,newdata=data[folds==i,])
    sum.sqerr[i] = sum((y[folds==i]-ypred)^2)
    sum.abserr[i] = sum(abs(y[folds==i]-ypred))
    sum.pererr[i] = sum(abs(y[folds==i]-ypred)/y[folds==i])
  }
  cv = return(data.frame(RMSEP=sqrt(sum(sum.sqerr)/n),
                         MAE=sum(sum.abserr)/n,
                         MAPE=sum(sum.pererr)/n))
}



bootlog.cv = function(fit,B=100,data=fit$model) {
  yt=fit$fitted.values+fit$residuals
  yact = exp(yt)
  yhat = exp(fit$fitted.values)
  resids = yact - yhat
  ASR=mean(resids^2)
  AAR=mean(abs(resids))
  APE=mean(abs(resids)/yact)
  boot.sqerr=rep(0,B)
  boot.abserr=rep(0,B)
  boot.perr=rep(0,B)
  y = fit$model[,1]
  x = fit$model[,-1]
  n = nrow(data)
  for (i in 1:B) {
    sam=sample(1:n,n,replace=T)
    samind=sort(unique(sam))
    temp=lm(formula(fit),data=data[sam,])
    ytp=predict(temp,newdata=data[-samind,])
    ypred = exp(ytp)
    boot.sqerr[i]=mean((exp(y[-samind])-ypred)^2)
    boot.abserr[i]=mean(abs(exp(y[-samind])-ypred))
    boot.perr[i]=mean(abs(exp(y[-samind])-ypred)/exp(y[-samind]))
  }
  ASRo=mean(boot.sqerr)
  AARo=mean(boot.abserr)
  APEo=mean(boot.perr)
  OPsq=.632*(ASRo-ASR)
  OPab=.632*(AARo-AAR)
  OPpe=.632*(APEo-APE)
  RMSEP=sqrt(ASR+OPsq)
  MAEP=AAR+OPab
  MAPEP=(APE+OPpe)*100
  cat("RMSEP\n")
  cat("===============\n")
  cat(RMSEP,"\n\n")
  cat("MAE\n")
  cat("===============\n")
  cat(MAEP,"\n\n")
  cat("MAPE\n")
  cat("===============\n")
  cat(MAPEP,"\n\n")
  return(data.frame(RMSEP=RMSEP,MAE=MAEP,MAPE=MAPEP))  
}


nnet.sscv = function(x,y,fit,data,p=.667,B=10,size=5,decay=.001,skip=F,linout=T,maxit=25000) {
  require(nnet)
  n = length(y)
  MSEP = rep(0,B)
  MAEP = rep(0,B)
  MAPEP = rep(0,B)
  ss = floor(n*p)
  for (i in 1:B) {
    sam = sample(1:n,ss,replace=F)
    fit2 = nnet(formula(fit),size=size,linout=linout,skip=skip,decay=decay,maxit=maxit,
                trace=F,data=data[sam,])
    yhat = predict(fit2,newdata=x[-sam,])
    ypred = exp(yhat)
    yact = exp(y[-sam])
    MSEP[i] = mean((ypred-yact)^2)
    MAEP[i] = mean(abs(ypred-yact))
    MAPEP[i] = mean(abs(ypred-yact)/yact)
  }
  RMSEP = sqrt(mean(MSEP))
  MAE = mean(MAEP)
  MAPE = mean(MAPEP)
  cat("RMSEP\n")
  cat("=============================\n")
  cat(RMSEP,"\n\n")
  cat("MAE\n")
  cat("=============================\n")
  cat(MAE,"\n\n")
  cat("MAPE\n")
  cat("=============================\n")
  cat(MAPE*100,"\n\n")
  temp = data.frame(RMSEP=sqrt(MSEP),MAEP=MAEP,MAPEP=MAPEP*100)
  return(temp)
}

#Monte Carlo Cross-Validation of Ridge and Lasso Regression
glmnet.ssmc = function(X,y,p=.667,M=100,alpha=1,lambda=1) {
  RMSEP = rep(0,M)
  MAEP = rep(0,M)
  MAPEP = rep(0,M)
  n = nrow(X)
  for (i in 1:M) {
    ss = floor(n*p)
    sam = sample(1:n,ss,replace=F)
    fit = glmnet(X[sam,],y[sam],lambda=lambda,alpha=alpha)
    ypred = predict(fit,newx=X[-sam,])
    RMSEP[i] = sqrt(mean((y[-sam]-ypred)^2))
    MAEP[i] = mean(abs(y[-sam]-ypred))
    yp = ypred[y[-sam]!=0]
    ya = y[-sam][y[-sam]!=0]
    MAPEP[i]=mean(abs(yp-ya)/ya)
  }
  cat("RMSEP =",mean(RMSEP),"  MAEP=",mean(MAEP),"  MAPEP=",mean(MAPEP))
  cv = return(data.frame(RMSEP=RMSEP,MAEP=MAEP,MAPEP=MAPEP)) 
}

#when response is logged
glmnet.sslog = function(X,y,p=.667,M=100,alpha=1,lambda=1) {
  RMSEP = rep(0,M)
  MAEP = rep(0,M)
  MAPEP = rep(0,M)
  n = nrow(X)
  for (i in 1:M) {
    ss = floor(n*p)
    sam = sample(1:n,ss,replace=F)
    fit = glmnet(X[sam,],y[sam],lambda=lambda,alpha=alpha)
    ypred = predict(fit,newx=X[-sam,])
    ya = exp(y[-sam])
    ypred = exp(ypred)
    RMSEP[i] = sqrt(mean((ya-ypred)^2))
    MAEP[i] = mean(abs(ya-ypred))
    MAPEP[i]=mean(abs(ypred-ya)/ya)
  }
  cat("RMSEP =",mean(RMSEP),"  MAEP=",mean(MAEP),"  MAPEP=",mean(MAPEP))
  cv = return(data.frame(RMSEP=RMSEP,MAEP=MAEP,MAPEP=MAPEP))
  
}


#Monte Carlo Cross-Validation of OLS Regression Models
MLR.ssmc = function(fit,p=.667,M=100) {
  RMSEP = rep(0,M)
  MAEP = rep(0,M)
  MAPEP = rep(0,M)
  y = fit$model[,1]
  x = fit$model[,-1]
  data = fit$model
  n = nrow(data)
  for (i in 1:M) {
    ss = floor(n*p)
    sam = sample(1:n,ss,replace=F)
    fit2 = lm(formula(fit),data=data[sam,])
    ypred = predict(fit2,newdata=x[-sam,])
    RMSEP[i] = sqrt(mean((y[-sam]-ypred)^2))
    MAEP[i] = mean(abs(y[-sam]-ypred))
    yp = ypred[y[-sam]!=0]
    ya = y[-sam][y[-sam]!=0]
    MAPEP[i]=mean(abs(yp-ya)/ya)
  }
  cat("RMSEP =",mean(RMSEP),"  MAEP=",mean(MAEP),"  MAPEP=",mean(MAPEP))
  cv = return(data.frame(RMSEP=RMSEP,MAEP=MAEP,MAPEP=MAPEP))
}



```

```{r}
#setwd("~/OneDrive - MNSCU/myGithub/Statistics/Regression_models/Multiple_Linear_Regression/MLR-ACE-AVAS-MARS/Shrinkage_Methods")
#Data prep
lu = read.csv("Lu2004.csv")
X = scale(model.matrix(Age~., data = lu)[,-1])
#Xs = scale(X)
y = lu$Age
```

This data consits of n=30 subjects and 403 columns that represent genetic marker intensity measurments for 403 different genes.

```{r}
#Explore the response variable
par(mfrow=c(1,1))
hist(lu$Age)
Statplot(y)
BCtran(lu$Age)
y.trans = bcPower(lu$Age, 0.5)
Statplot(y.trans)
```

## Sequence of ridge, Lasso, Elastic Net regression models with 'grid' values.

```{r fig.height=15, fig.width=15}
grid = 10^seq(10,-2,length = 200)
par(mfrow=c(3,2))
#ridge
ridge.mod = glmnet(X,y,alpha=0,lambda=grid)
plot(ridge.mod)
title(main ="Coefficient Shrinkage using Ridge Regression Model")
plot(ridge.mod,xvar="lambda")
title(main ="Coefficient Shrinkage using Ridge Regression Model")

#lasso
lasso.mod = glmnet(X,y,alpha=1,lambda=grid)
plot(lasso.mod)
title(main ="Coefficient Shrinkage using Lasso Regression Model")
plot(lasso.mod, xvar = "lambda")
title(main ="Coefficient Shrinkage using Lasso Regression Model")
#elastic
en.mod = glmnet(X,y,alpha=0.5, lambda = grid)
plot(en.mod)
title(main ="Coefficient Shrinkage using Elastic Net Regression Model")
plot(en.mod,xvar="lambda")
title(main ="Coefficient Shrinkage using Elastic Net Regression Model")
```

In the figures above, we will focus on the Log Lambda scale - 3 graphs in 2nd column, reading top to bottom. The higher the Log lambda, the closer we are to OLS plot.
Ridge method, 1st row: it seems to have a couple (a few) of very important variables streching to the sides. Log(lambda) is going to be between 4-6 just by looking at this graph.

Lasso mehtod, 2nd row: we see more important variables, and a lot of them that are around coefficient value = 0 are irrelevant; it looks like Lasso turned this model into a bunch of very useful variables vs. irrelevant ones.

Third row, in terms of interpretibality, Elastic Net looks similiar to Lasso.

We can say that higher values (larger positives or larger negatives) of coefficients of those genes are more useful in determining the age of the subject. Without further information about gene functions, we cannot interpret how the relevant genetic marker intensity measurments - chosen by our model - relate to predicting the age of the subject. Nevertheless, thanks to the model, we know which such genes are most related/useful. 

In the next steps, we are going to find best predicting method by comparing the three regression shrinkage methods: ridge, Lasso, and Elastic Net.



## Optimal lambda for ridge, Lasso, Elastic Net

Finding  lambda for ridge, Lasso, and Elastic Net regression using the cv.glmnet function - including plots of the cross-validation results for all methods.



```{r fig.height=5, fig.width=15}
#Ridge regression
par(mfrow=c(1,2))
cv.ridge= cv.glmnet(X, y, alpha = 0)
bestlam.ridge = cv.ridge$lambda.min
plot(cv.ridge)
title(main = paste("Best log(lambda) for Ridge", (round(log(bestlam.ridge),digits = 2))), sub = paste("Best lambda:", round(bestlam.ridge)))
#ridge.results = glmnet.ssmc(X,y, p=.75, M=1000,alpha=0,lambda=bestlam.ridge)

#Transformed resposne ridge regression
y.back.trans = invBoxCox(y.trans, 0.5)
cv.ridge.trans= cv.glmnet(X, y.trans, alpha = 0)
bestlam.ridge.trans = cv.ridge.trans$lambda.min
plot(cv.ridge.trans)
title(main = paste("Best log(lambda) for Ridge with transformed resposne", (round(log(bestlam.ridge.trans), digits = 2))), sub = paste("Best lambda:", round(bestlam.ridge.trans)))
#ridge.trans.results = glmnet.ssmc(X,y.back.trans,p=.75,M=1000,alpha=0,lambda=bestlam.ridge.log)
```

The optimal lambda found by cv.glmnet function is shown in the plots above. The vertical dashed lines across the entire graphic represent lower bound and upper bound limits of optimal lambda values. The optimal (minimal) lambda value has the lowest Mean Squared Erroe (MSE). For when the response variable (age) is in its origina scale, best lambda is log(5.93); when the response is transformed, best log(lambda) is 4.09. Log(lambda) is displayed for comparison purposes to the x-axis on the graphs. At the bottom of the plots, the actual value of best lambda is displayed.

The above regards as well the two following figures for Lasso and Elastic Net methods.


```{r fig.height=5, fig.width=15}
par(mfrow=c(1,2))
#Lasso regression
cv.lasso = cv.glmnet(X, y, alpha = 1)
bestlam.lasso = cv.lasso$lambda.min
plot(cv.lasso)
title(main = paste("Best log(lambda) for Lasso", (round(log(bestlam.lasso),digits = 4))), sub = paste("Best lambda:", round(bestlam.lasso, 4)))


#lasso.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=1,lambda=bestlam.lasso)

#Transformed response lasso 
cv.lasso.trans = cv.glmnet(X, y.trans, alpha = 1)
bestlam.lasso.trans = cv.lasso.trans$lambda.min
plot(cv.lasso.trans)
title(main = paste("Best log(lambda) for Lasso with transformed resposne", (round(log(bestlam.lasso.trans),digits = 4))), sub = paste("Best lambda:", round(bestlam.lasso.trans,4)))
#lasso.trans.results = glmnet.ssmc(X,y.back.trans,p=.75,M=1000,alpha=1,lambda=bestlam.lasso.trans)
#basing it on MAPE, we're choosing the log reposne
```


```{r fig.height=5, fig.width=15}
par(mfrow=c(1,2))
#Elastic net
cv.en = cv.glmnet(X,y,alpha=0.15)
bestlam.en = cv.en$lambda.min
plot(cv.en)
title(main = paste("Best log(lambda) for Elastic Net", (round(log(bestlam.en),digits = 2))), sub = paste("Best lambda:", round(bestlam.en,4)))
#en.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=0.1,lambda=bestlam.en)

#Transformed response en 
cv.en.trans = cv.glmnet(X,y.trans,alpha=0.15)
bestlam.en.trans = cv.en.trans$lambda.min
plot(cv.en.trans)
title(main = paste("Best log(lambda) for Elastic Net with response transformed", (round(log(bestlam.en.trans),digits = 2))), sub = paste("Best lambda:", round(bestlam.en.trans,4)))
#en.trans.results = glmnet.ssmc(X,y.back.trans, M=1000,alpha=0.1,lambda=bestlam.en.trans)
```


## Fit the optimal ridge, Lasso, and Elastic Net regression models and construct plots of the predicted ages (y ̂) vs. actual age (y)
```{r fig.height=15, fig.width=10}
par(mfrow=c(3,1))

ridge.mod = glmnet(X,y,alpha=0,lambda=bestlam.ridge)
#y and yhat correlation
ridge.cor = cor(y, predict(ridge.mod, newx = X))
#y and yhat correlation^2 = R^2
ridge.rsqaured = cor(y, predict(ridge.mod, newx = X))^2
plot(y,predict(ridge.mod,newx=X),xlab="Actual Age (y-values)",ylab="Predicted Age (yhat-values)", main = paste("Ridge Model:", "   ", "Correlation:", round(ridge.cor,4), "   ", "R^2:", round(ridge.rsqaured,4)))


lasso.mod = glmnet(X,y,alpha=1,lambda=bestlam.lasso)
#y and yhat correlation
lasso.cor = cor(y, predict(lasso.mod, newx = X))
#y and yhat correlation^2 = R^2
lasso.rsqaured = cor(y, predict(lasso.mod, newx = X))^2
plot(y,predict(lasso.mod,newx=X),xlab="Actual Age (y-values)",ylab="Predicted Age (yhat-values)", main = paste("Lasso Model:", "   ", "Correlation:", round(lasso.cor,4), "   ", "R^2:", round(lasso.rsqaured,4)))

en.mod = glmnet(X,y,alpha=0.15, lambda =bestlam.en)
#y and yhat correlation
en.cor = cor(y, predict(en.mod, newx = X))
#y and yhat correlation^2 = R^2
en.rsqaured = cor(y, predict(en.mod, newx = X))^2
plot(y,predict(en.mod,newx=X),xlab="Actual Age (y-values)",ylab="Predicted Age (yhat-values)", main = paste("Elastic Net Model:", "   ", "Correlation:", round(en.cor,4), "   ", "R^2:", round(en.rsqaured,4)))
```

The best model predicting age by far is the Elastic Net model with lambda = 1.96 (bestlam.en). However, as we will see later, this model suffers from overfitting. When we perform a Monte-Carlo cross validation, we will notice that our model is picking up the noise and random fluctuactions in the data that it performs in it too well. We may want to regularize the penalty term, alpha. Increasing alpha in the elastic net model may help adjust high variance issue (alpha=0.15).

## Which genes are most related or useful in determining the age of the subject?

I.e., examine and interpret estimated coefficients of the model chosen above.
```{r}
#Coefficients' importance for Elastic Net
en.coef = coef(en.mod, s=bestlam.en)
en.coef[order(en.coef, decreasing = TRUE)]
```
Here is the list of the most important genes when using Elastic Net (2-11 and 394-404):
```{r}
par(mfrow=c(1,1))
important_genes_en = lu[,c(2:5, 397:404)]
names(important_genes_en)
en.mod = glmnet(X,y,alpha=0.5, lambda = grid)
plot(en.mod,xvar="lambda")
title(main ="Coefficient Shrinkage using Elastic Net Regression Model")
```

The list of the genes mentioned above is represented by the variables that stretch to the sides the most – away from line 0.

```{r}
#Coefficients' importance for Lasso
ridge.coef = coef(lasso.mod, s = bestlam.lasso)
ridge.coef[order(ridge.coef, decreasing = TRUE)]
```
The most important genes are from: 2-6 and 401-404
```{r}
important_genes_lasso = lu[,c(2:6, 401:404)]
names(important_genes_lasso)
```


```{r}
lasso.mod = glmnet(X,y,alpha=1,lambda=grid)
plot(lasso.mod, xvar = "lambda")
title(main ="Coefficient Shrinkage using Lasso Regression Model")
```

The list of the genes mentioned above is represented by the variables that stretch to the sides the most – away from line 0.

## Monte Carlo cross-validation to estimate the prediction accuracies for ridge, Lasso, and Elastic Net regression

```{r}
ridge.results = glmnet.ssmc(X,y, p=.75, M=1000,alpha=0,lambda=bestlam.ridge)
ridge.trans.results = glmnet.ssmc(X,y.back.trans,p=.75,M=1000,alpha=0,lambda=bestlam.ridge.trans)
lasso.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=1,lambda=bestlam.lasso)
lasso.trans.results = glmnet.ssmc(X,y.back.trans,p=.75,M=1000,alpha=1,lambda=bestlam.lasso.trans)
en.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=0.15,lambda=bestlam.en)
en.trans.results = glmnet.ssmc(X,y.back.trans,p=.75, M=1000,alpha=0.15,lambda=bestlam.en.trans)
```


```{r}
#Compare the three methods
names = c("Ridge", "Ridge Transformed", "Lasso", "Lasso Transformed", "Elastic Net", "Elastic Net Transormed")
metrics =cbind(((names)), rbind(
  do.call(cbind, lapply(ridge.results,mean)), 
  do.call(cbind, lapply(ridge.trans.results, mean)),
  do.call(cbind, lapply(lasso.results, mean)),
  do.call(cbind, lapply(lasso.trans.results, mean)),
  do.call(cbind, lapply(en.results, mean)),
  do.call(cbind, lapply(en.trans.results, mean))))
```

```{r}
as.data.frame(metrics)
```

As seen above, based on MAPE to best compare across metrics from other models, the best method was ridge and Elastic Net came in second best - however, the Elastic Net method was very close to the Lasso method in terms of determining the age of the subject given their genes. Recalling our Actual vs. Predicted plots from before, Elastic Net's fit was outstanding; nevertheless, as seen from out cross-validation accuracy metrics results, that model had high variance. We would further recommend to experiment with Elastic Net hyperparameter tuning.
