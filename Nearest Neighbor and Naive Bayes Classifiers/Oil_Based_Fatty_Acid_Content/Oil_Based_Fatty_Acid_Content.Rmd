---
title: "Oils"
author: "Sam and Mikolaj"
date: "4/14/2020"
output:
  md_document:
    variant: markdown_github
---


# TYPE OF OIL BASED FATTY ACID CONTENT

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#Libraries:
require(kknn)
require(class)
require(klaR)
require(e1071)
require(s20x)
```

## Data

The file <b>Oils.csv</b> contains fatty acid content readings for samples of seven types of oils (pumpkin, sunflower, peanut, olive, soybean, rapeseed, and corn).  The fatty acids measured are: Palmitic, Stearic, Oleic, Linoleic, Linolenic, Eicosanoic, and Eicosenoic.  

```{r}
setwd(getwd())
Oils =read.csv("Oils.csv")
summary(Oils)
```

## Data Preparation

### Train/Validation Split

It wouldn't be wise to split the data into train and validation sets as we only have a total of 96 observations in the original dataset. The decision making factor for not splitting the data is
the amount of observations representing each class. There is 7 classes of Type categorical variable. Splitting the dataset with stratifying the Type variable could be a potential problem-solver as it would keep the same proportion of observations belonging to each class. However, as we can see (below) the low level of counts of each class below, it is not doable to make each cateogry representative of the data if we attempted a train/validation set split.

```{r}
summary(Oils$Type)
```

### Standardizing 

All our predicotrs are numeric variables. Their scales differ a little bit, and as we are going to be using k-nearest neighbors to classify our observations, we will be working with straight line distance metrics - so we are going to standardize our data (mean=0, stdev=1). It is crucial when using k-nearest neighbors approach.

```{r}
Oils[,2:8] = scale(Oils[,2:8])
set.seed(888)
```

## Model building

We will be using k-NN and Na√Øve Bayes to classify the training data. 

### Unweighted kNN

Let's start with a simple, unweighted k-NN classification model. For this, we will use sknn() function from the klaR library.

Gamma parameter is a tunning parameter. Gamma = 0 gives us the unweighted model. As gamma increases, the more down-weighted we get, with weights dropping very fast.

```{r}
#Nearest Neighbors (k=1)
oils.sknn = sknn(Type~., data = Oils, kn=1)
```

```{r}
ypred = predict(oils.sknn)
```

predict() function returns two attributes: posterior and class. Posterior is a predicted probability of the observation being in each class; class gives us a predicted class.

Let's look at the missclassfication rate using the following function:
```{r}
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
```


```{r}
#Nearest Neighbors (k=1)
misclass(ypred$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=2)
oils.sknn = sknn(Type~., data = Oils, kn=2)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=3)
oils.sknn = sknn(Type~., data = Oils, kn=3)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=4)
oils.sknn = sknn(Type~., data = Oils, kn=4)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=5)
oils.sknn = sknn(Type~., data = Oils, kn=5)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=6)
oils.sknn = sknn(Type~., data = Oils, kn=6)
misclass(predict(oils.sknn)$class,Oils$Type)
```

The model with k=1 nearest neighbor is performing the best. However, when we use 1-nearest neighbor classifier, there is a high likelihood we are encountering overfitting of our data. Estimating the probability based on a single sample (first closest neighbor) is sensitive to noise, missclassification, When we use larger k values, we increase the robustness of our model, as then our decision surface gets smoother. However, for larger k values, our missclassificaiton rate was increasing. 

k=2 and k=3 were having the same performance. We are going to keep kn=3 in our model.
Let's implement weighing into our model and see if that can improve it. We are not going to be considering kn=1 from now on.

### Weighted kNN

```{r}
#Nearest Neighbors (k=2, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma = 1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=3, gamma=2)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma=2)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=4, gamma=3)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma = 3)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=5, gamma=4)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma=4)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=4, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=4, gamma=1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=5, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=5, gamma=1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=6, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=6, gamma=1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=6, gamma=2)
oils.sknn = sknn(Type~., data = Oils, kn=6, gamma=2)
misclass(predict(oils.sknn)$class,Oils$Type)
```

As seen above, using any combination of the number of nearest neighbors (kn = {1,6}) and gamma parameter (weighing), our missclassification rate is 0.

### Distance metric Tunning

Let's experiment with Minkowski distance metric.

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("optimal"), distance = 1)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("optimal"), distance = 2)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("optimal"), distance = 3)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

Putting more emphasis on Mikowski distance isn't chaning the missclassificaiton rate, perhaps modifying the kernel will make a difference.

### Kernel Tunning

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("triangular"), distance = 1, gamma = 1)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c( "rectangular", "epanechnikov", "optimal", "triweight")) 
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3,kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "triweight", "gaussian"), distance = 1)
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

Any of the weighted knn approaches give <b>NO missclassifications</b> on the training data. However we cannot know whether our model could be underfitting (having a high bias and low variance) or overfitting (low bias and high variance) as we are not predicting on a different training data set (validation set).

### Parameter Grid Search

As seen above, there is many many possible combinations to find optimal paramters. Let's use a very handy parameter grid search tool to have it find the best combination of tunning parameters for us.

This function tune() is in the package called "e1071"

First, we need to form our resposne vector 'y' and a data frame 'x' of our predictors.

```{r}
y = Oils[,1]
x = Oils[,-1]
```

Here, we have to be using knn() function to fit kNN classifier. Parameter 'l' specifies the minimum vote for the definite decision when there is a tie. If it's null, there is a doubt (Unkown "?").

```{r}
require(e1071)
tuned = tune.knn(x,y, k=2:15, tunecontrol = tune.control(sampling = "boot"), data = Oils)
summary(tuned)
```

Seems like lower k values are classifying our problem better. Also, this function is not so useful as intially thought - it does not search for optimal weighting gamma parameter or kernels. It is good, however, for a quick look at the performance of different number of nearest neighbors.

### Monte-Carlo Cross Validation for k-NN

As we mentioned above, Cross Validation is very important to measure accuracy of our model and see whether we have an optimal bias-variance trade-off.

We will randomly split the Oils data B times into training and validation sets using Monte-Carlo Cross Validation functions below.

```{r}
#For uing kknn()
kknn.cv = function(train,y=train[,1],B=25,p=.333,kmax=3,kernel="optimal",distance=2) {
  y = as.factor(y)
  data = data.frame(y=y,train[,-1])
  n = length(y)
  cv <- rep(0,B)
  leaveout = floor(n*p)
  for (i in 1:B) {
    sam <- sample(1:n,leaveout,replace=F)
    fit <- train.kknn(y~.,data=data[-sam,],kmax=kmax,kernel=kernel,distance=distance)
    ypred = predict(fit,newdata=data[sam,])
    tab <- table(y[sam],ypred)
    mc <- leaveout - sum(diag(tab))
    cv[i] <- mc/leaveout
  }
  cv
}

#For using sknn()
sknn.cv = function(train,y=train[,1],B=25,p=.333,k=3,gamma=0) {
y = as.factor(y)
data = data.frame(y,train)
n = length(y)
cv <- rep(0,B)
leaveout = floor(n*p)
for (i in 1:B) {
        sam <- sample(1:n,leaveout,replace=F)
        temp <- data[-sam,]
        fit <- sknn(y~.,data=temp,kn=k,gamma=gamma)
        pred = predict(fit,newdata=train[sam,])$class
        tab <- table(y[sam],pred)
        mc <- leaveout - sum(diag(tab))
        cv[i] <- mc/leaveout
        }
  cv
}


```

The data frame needs to have the response as the first variable for the functions above.

#### For sknn

train,y=train[,1],B=25,p=.333,k=3,gamma=0)
```{r}
set.seed(888)
results = sknn.cv(Oils,B=25,k=3,gamma=0)
summary(results)
```

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=3,gamma=1)
summary(results)
```

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=3,gamma=2)
summary(results)
```

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=5,gamma=2)
summary(results)
```

Best metrics for k=3 with gamma=2 (M=0.01032) and k=5 with gamma=2 (M=0.0129)

#### For kknn

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=15,kernel="optimal")
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=4,kernel="triangular")
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=10,kernel="triangular",distance=1) 
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=15,kmax=5,kernel="triangular",distance=1) 
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=10,kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "triweight", "gaussian"),distance=1)
summary(results)
```

Metrics for sknn were of better performance.


### Naive Bayes

Which type of Oils has the highest posterior probability of being observed given the data?
fL = tunning parameter Laplace


#### Normal Distribution

Which of the fatty acids have the best discriminatory ability in classying which type of Oil it is? Let's look at two examples.

```{r warning=F}
boxqq(Palmitic~Type, data=Oils)
```

High levels are associated with the given type. For example, for Palmitic fatty acid, we can see it would most likley be associated with Type D. And vice-versa, if the Linoleic levels are low, there is a good chance, they are associated with Type D (as seen below).

For Palmitic, there is a tiny bit of right skewness for Type B and left skewness for Type A and D (with D seeming to have an outlier). 

```{r warning=F}
boxqq(Linoleic~Type, data=Oils)
```

For Linoleic fatty acid content, normality looks pretty okay across the seven Type categories (Type F looks a bit skewed right).


```{r warning=F}
boxqq(Stearic~Type, data=Oils)
```

For Stearic fatty acid content, we see that the Type A and B are right skewed.


```{r warning=F}
boxqq(Oleic~Type, data=Oils)
```

For Oleic fatty acid content, we see that the Types follow the normal QQ reference lines (Type D seems left skewed)

```{r warning=F}
boxqq(Linolenic~Type, data=Oils)
```

Here, for Linolenic, Type F seems to be right skewed.


```{r warning=FALSE}
boxqq(Eicosanoic~Type, data=Oils)
```

For Eicosanoic fatty acid content, we can see that for Type A, there is a huge outlier. Type B, E, and F are skewed left and Type D is skewed right.

```{r warning=F}
boxqq(Eicosenoic~Type, data=Oils)
```

Normality of Eicosenoic looks like being left skewed for Type F and right skewed for Type C.


We keep everything standardized (mean=0 and stddev=1)
```{r}
require(s20x)
set.seed(888)
oils.nb = naiveBayes(Type~., data = Oils, laplace = 1)
ypred = predict(oils.nb, newdata = Oils[,-1], type = "raw") # type="raw" is giving probability
head(ypred)
```

Giving the estimated probabiliy for each of the observation being each Type. We want to chose the one that is the largest. 

Observation from 1 to 6 given above (as rows). The first one gets classified as being of Type G, the second one gets classified as being of Type D, the third one gets classified as being of Type E, the fourth one gets classified as being of Type D, the fifth one gets classified as being of Type E, and the sixth one gets classified as being of Type F.

--The interpretation would work like that if there wasn't the beautiful and odd Type A column - it is classifying everything to its Type A with probabilites being 1...?

If you do not type in type="raw" in predict() we would get teh acutal class the predicates belong to. Let's look into the missclassification rate:

```{r}
ypred = predict(oils.nb, newdata = Oils[,-1]) #getting classes
misclass(ypred,Oils$Type)
```

We are getting the same missclassification rate (1.04%) at this point as we were getting with the unweighted kNN (k=3).

### Monte-Carlo Cross Validation for Naive Bayes
```{r}
nB.cv = function(X,y,B=25,p=.333,laplace=0) {
y = as.factor(y)
data = data.frame(y,X)
n = length(y)
cv <- rep(0,B)
leaveout = floor(n*p)
for (i in 1:B) {
        sam <- sample(1:n,leaveout,replace=F)
        temp <- data[-sam,]
        fit <- naiveBayes(y~.,data=temp,laplace=laplace)
        pred = predict(fit,newdata=X[sam,])
        tab <- table(y[sam],pred)
        mc <- leaveout - sum(diag(tab))
        cv[i] <- mc/leaveout
        }
  cv
}
```

Let's use the Monte-Carlo Cross-Validation, randomly splitting Oils data B times.

```{r}
set.seed(888)
X = Oils[,-1]
y = Oils[,1]
results = nB.cv(X, y, B=500, laplace = 1)
summary(results)
```

On average, we are getting about 8.6% missclassified cases.

### kNN final model:

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=3,gamma=1)
summary(results)
```


```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=5,gamma=2)
summary(results)
```

Missclassification rate (on average), 2.09% (k=3, gamma=1) and 1.97% (k=5, gamma=2)

### Naive Bayes final mode:

```{r}
set.seed(888)
X = Oils[,-1]
y = Oils[,1]
results = nB.cv(X, y, B=500, laplace = 1)
summary(results)
```

Missclassification rate (on average), 8.6%

Naive Bayes may be more suited for categorical predictors, not numeric ones (our Oils data has all numeric predictors).

k-NN is the winner (the data needs to have more observations to ensure quality of representation of each Type category).



