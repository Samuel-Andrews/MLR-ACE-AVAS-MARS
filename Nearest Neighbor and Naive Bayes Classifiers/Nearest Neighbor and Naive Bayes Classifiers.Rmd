---
title: "Nearest Neighbor and Naive Bayes"
author: "Samuel A, Mikolaj W"
date: "April 14, 2020"
output: word_document
---

# SATELLITE IMAGE DATA

The goal here is to predict the type of ground cover from a satellite image broken up into pixels.
Description from UCI Machine Learning database:

The database consists of the multi-spectral values of pixels in 3x3 neighborhoods in a satellite image, and the classification associated with the central pixel in each neighborhood. The aim is to predict this classification, given the multi-spectral values. In the sample database, the class of a pixel is coded as a number. 

The Landsat satellite data is one of the many sources of information available for a scene. The interpretation of a scene by integrating spatial data of diverse types and resolutions including multispectral and radar data, maps indicating topography, land use etc. is expected to assume significant importance with the onset of an era characterized by integrative approaches to remote sensing (for example, NASA's Earth Observing System commencing this decade). Existing statistical methods are ill-equipped for handling such diverse data types. Note that this is not true for Landsat MSS data considered in isolation (as in this sample database). This data satisfies the important requirements of being numerical and at a single resolution, and standard maximum-likelihood classification performs very well. Consequently, for this data, it should be interesting to compare the performance of other methods against the statistical approach. 

One frame of Landsat MSS imagery consists of four digital images of the same scene in different spectral bands. Two of these are in the visible region (corresponding approximately to green and red regions of the visible spectrum) and two are in the (near) infra-red. Each pixel is a 8-bit binary word, with 0 corresponding to black and 255 to white. The spatial resolution of a pixel is about 80m x 80m. Each image contains 2340 x 3380 such pixels. 

The database is a (tiny) sub-area of a scene, consisting of 82 x 100 pixels. Each line of data corresponds to a 3x3 square neighborhood of pixels completely contained within the 82x100 sub-area. Each line contains the pixel values in the four spectral bands (converted to ASCII) of each of the 9 pixels in the 3x3 neighborhood and a number indicating the classification label of the central pixel. The number is a code for the following classes: 

Number Class 
1 red soil 
2 cotton crop 
3 grey soil 
4 damp grey soil 
5 soil with vegetation stubble 
6 mixture class (all types present)  
7 very damp grey soil 

Note: There are no examples with class 6 in this dataset. 

The data is given in random order and certain lines of data have been removed so you cannot reconstruct the original image from this dataset. 

In each line of data the four spectral values for the top-left pixel are given first followed by the four spectral values for the top-middle pixel and then those for the top-right pixel, and so on with the pixels read out in sequence left-to-right and top-to-bottom. Thus, the four spectral values for the central pixel are given by attributes 17,18,19 and 20. 

```{r}
setwd(getwd())
SATimage = read.csv("SATimage.csv")
SATimage = data.frame(class=as.factor(SATimage$class),SATimage[,1:36])
```

This command makes sure that the response is interpreted as a factor (categorical) rather than as a number.  Use the SATimage as the data frame throughout.

Create a test and training set using the code below:

```{r}
set.seed(888)
testcases = sample(1:dim(SATimage)[1],1000,replace=F)
SATtest = SATimage[testcases,]
SATtrain = SATimage[-testcases,]
```



## Comparing k-NN classification and Na√Øve Bayes classification for predicting the test cases in SATtest

Mounting Libraries

```{r message=FALSE, warning=FALSE}
require(kknn)
require(class)
require(klaR)
```

Nearest Neighbor classification missclassification function:

```{r}
misclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}

```


Scaling was considered, but since the variables are all in the same scale, it was deemed unescescary.


### Nearest Neighbor Classification:

Starting with just a simple kn = 1 model
```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=1)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)

```

Even before putting work, we are only above wrong about 10% of the time, which for minimal effort is quite impressive. However, we can still attempt to improve this by modifying additional features.

First, we will attempt to optimize our number of neighbors.
```{r}

sat.sknn = sknn(class~.,data=SATtrain,kn=2)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)

```


```{r}

sat.sknn = sknn(class~.,data=SATtrain,kn=3)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)
```


```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=4)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)

```


```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=5)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)
```


```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=6)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)
```


```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=7)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)
```


```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=8)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)
```


```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=9)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)
```


```{r}
sat.sknn = sknn(class~.,data=SATtrain,kn=10)
yhat = predict(sat.sknn,newdata=SATtest)

misclass(yhat$class,SATtest$class)
```

Incresing the number of nearest neighbors beyond this doesn't make a lot of sense given the pattern were seeing. While the imporvment is around 1%, kn = 3 performs the best.


```{r}
sat.sknn3 = sknn(class~.,data=SATtrain,kn=3)
yhat3 = predict(sat.sknn,newdata=SATtest)

```

Next, weighting will tried in a manner similar to more traditional nearest neighbor regression.


```{r}
sat.sknn3w = sknn(class~.,data=SATtrain,kn=3, gamma = 1)
yhat3w = predict(sat.sknn3w,newdata=SATtest)

misclass(yhat3w$class,SATtest$class)
```
Immediate results aren't boading well, but perhaps this changes with weight increases.


```{r}
sat.sknn3w = sknn(class~.,data=SATtrain,kn=3, gamma = 2)
yhat3w = predict(sat.sknn3w,newdata=SATtest)

misclass(yhat3w$class,SATtest$class)
```


```{r}
sat.sknn3w = sknn(class~.,data=SATtrain,kn=3, gamma = 3)
yhat3w = predict(sat.sknn3w,newdata=SATtest)

misclass(yhat3w$class,SATtest$class)
```


```{r}
sat.sknn3w = sknn(class~.,data=SATtrain,kn=3, gamma = 4)
yhat3w = predict(sat.sknn3w,newdata=SATtest)

misclass(yhat3w$class,SATtest$class)
```


```{r}
sat.sknn3w = sknn(class~.,data=SATtrain,kn=3, gamma = 4)
yhat3w = predict(sat.sknn3w,newdata=SATtest)

misclass(yhat3w$class,SATtest$class)
```

```{r}
sat.sknn3w = sknn(class~.,data=SATtrain,kn=3, gamma = 5)
yhat3w = predict(sat.sknn3w,newdata=SATtest)

misclass(yhat3w$class,SATtest$class)
```

Weighting clearly isn't the answer, as with rbf up to 5 we almost are up to predicintingg wrong everytime, with any weight at all doubling our missclassification rate. Perhaps by using the manhattan or taxi cab metric we can see some actual improvement.

```{r}
 
sat.knnD = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1)  
yhatD = predict(sat.knnD,newdata=SATtest)

misclass(yhatD,SATtest$class)

```


```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)

```

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 3)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)

```

Putting more emphasis on Mikowski distance isn't helping, perhaps modifying the kernel will make a difference.

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=1, kernel=c("triangular", "epanechnikov", "optimal"), distance = 3)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```

removing rectangular did not help, even being the simplest.

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight"), distance = 1)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```

Adding biweight as an additional helped, but what if triweight pushes it further?

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight", "triweight"), distance = 1)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```

It does the same. What if we do just triweight?

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "triweight"), distance = 1)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```

Having both was definetly holding us back.



```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "triweight", "gaussian"), distance = 1)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```

Guassian as an addition did not help.

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "triweight", "cos"), distance = 1)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```

A similar sotry with a cos addition. Perhaps we need to make the model simpler.

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c( "rectangular", "epanechnikov", "optimal", "triweight"), distance = 1)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```


This leaves our best model as a triweghit kn = 3 model with Minkowski distance of 1

```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triweight"), distance = 1)  
yhatD2 = predict(sat.knnD2,newdata=SATtest)

misclass(yhatD2,SATtest$class)
```

Overall, from the simplest model, we improved classification performance by around 2%.


### Naive Bayes:

Packages
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
require(e1071)
require(s20x)
```

Given the high number of variables we are working with, checking all of them for normality is not practical. It will be far more efficent to simply run the analysis assuming both potential conditions and see which performs better.

Starting simple with klar's implementation.

```{r}
sat.NB = NaiveBayes(class~.,data=SATtrain)

summary(sat.NB)
```


```{r message=FALSE, warning=FALSE}
yprednb = predict(sat.NB,newdata=SATtest)

misclass(yprednb$class,SATtest$class)


```


Given that assuming a conditional normal distribution for each predictor lead us to a result with more twice the missclassification rate of even the simplest nearest neighbor model, we likely are oging to want to consider more advnaced models using smoothers.


```{r message=FALSE, warning=FALSE}
sat.NB = NaiveBayes(class~.,data=SATtrain, usekernel = T)
yprednb = predict(sat.NB,newdata=SATtest)

misclass(yprednb$class,SATtest$class)

```

An improvement for sure, but still far behind a result we would consider good. Perhaps if we force the kernel to be more smooth we will get a result more desireable.


```{r message=FALSE, warning=FALSE}
sat.NB = NaiveBayes(class~.,data=SATtrain, usekernel = T, bw = T)
yprednb = predict(sat.NB,newdata=SATtest)

misclass(yprednb$class,SATtest$class)

```

No difference is made apparent. Perhaps a different library, such as e1071, has a slight edge over klar for this problem.


```{r}
sat.NB = naiveBayes(class~.,data=SATtrain)
yprednb = predict(sat.NB,newdata=SATtest)

misclass(yprednb,SATtest$class)
```


It seems that it made no difference. Our kernel smoothed Klar model comes out on top.


Final Models:

Nearest-Neighbor:
```{r}
sat.knnD2 = train.kknn(class~.,data=SATtrain,kn=3, kernel=c("triweight"), distance = 1)
```
Missclassification Rate = 8.3%

Naive-Bayes:
```{r}
sat.NB = NaiveBayes(class~.,data=SATtrain, usekernel = T, bw = T)

```
Missclassification Rate = 18.3%


## Monte Carlo Cross-Validation

Using Split-Sample Monte Carlo cross-validation to compare k-NN and Na√Øve Bayes to compare these methods of classification. 

First, we will look at our k-NN model.

### Monte Carlo Function for k-NN:

```{r}
kknn.sscv = function(train,y=train[,1],B=25,p=.333,kmax=3,kernel="optimal",distance=2) {
  y = as.factor(y)
  data = data.frame(y=y,train[,-1])
  n = length(y)
  cv <- rep(0,B)
  leaveout = floor(n*p)
  for (i in 1:B) {
    sam <- sample(1:n,leaveout,replace=F)
    fit <- train.kknn(y~.,data=data[-sam,],kmax=kmax,kernel=kernel,distance=distance)
    ypred = predict(fit,newdata=data[sam,])
    tab <- table(y[sam],ypred)
    mc <- leaveout - sum(diag(tab))
    cv[i] <- mc/leaveout
  }
  cv
}

```

Result:

```{r}
#results = kknn.sscv(SATimage,B=100,kmax=3,kernel="triweight", distance = 1, p = 0.33)

#summary(results)
```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
0.08808 0.09756 0.10196 0.10159 0.10586 0.11856 

Over 100 iterations, it seems that our observed metric of 8.3% missclassification was on the low end for a model of that type. Still, averaging about 11% is not horrible, but the result was not what one might have fully hoped for.


### Monte Carlo Function for Naive Bayes
```{r}
nB.cv = function(X,y,B=25,p=.333,laplace=0) {
y = as.factor(y)
data = data.frame(y,X)
n = length(y)
cv <- rep(0,B)
leaveout = floor(n*p)
for (i in 1:B) {
        sam <- sample(1:n,leaveout,replace=F)
        temp <- data[-sam,]
        fit <- naiveBayes(y~.,data=temp,laplace=laplace)
        pred = predict(fit,newdata=X[sam,])
        tab <- table(y[sam],pred)
        mc <- leaveout - sum(diag(tab))
        cv[i] <- mc/leaveout
        }
  cv
}

```

Result:

```{r}
#results2 = nB.cv(SATimage[,-1],SATimage[,1],B=100)

#summary(results2)
```
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
 0.1809  0.1963  0.2033  0.2031  0.2087  0.2256 

Naive-Bayes peformed similarly, being slightly worse through 100 passes in a Monte Carlo function. However, it still holds true that our k-NN model is roughly twice as effective than Naive-Bayes using missclassification as our chosen metric, with the average pass for Bayes garnering wrong answers anout 20% of the time compared to 10% from k-NN.


# CLASSIFYING MUSIC GENRE BASED ON AN AUDIO SAMPLE

This dataset consists of 191 continuous variables measured from an audio sample from a piece of music.  All audio samples were of the same length timewise.  Your goal is to use these data to classify the genre of the piece of music into one of the following genres: Blues, Classical, Jazz, Metal, Pop, and Rock.  These data are contained in two files: GenreTrain.csv (n = 10,000 samples) and GenreTest.csv (m = 2,495 samples).   


Reading in the data and scaling it.

```{r}
gtrain = read.csv("GenreTrain.csv")
gtest = read.csv("GenreTest.csv")

gtrain.X = scale(gtrain[,-192])

gtrain = data.frame(GENRE=gtrain$GENRE,gtrain.X)


gtest.X = scale(gtest[,-192])

gtest = data.frame(GENRE=gtest$GENRE,gtest.X)


```

Given that we have 10,000 0bservations for our training set, for model developement purposes we should be safe to split it further into a validation set and training set.

```{r}
testcases2 = sample(1:dim(SATimage)[1],3333,replace=F)
gval = gtrain[testcases2,]
gtrain = gtrain[-testcases2,]
```

Starting out simple, we will use a procedure similar to the problem above to build an optimal k-NN model.

### Nearest Neighbor Classification

Simple kn = 1 model:
```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)

```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       393         1   15     7   4    2
  Classical     5       872  100     4   4    6
  Jazz          4        30  658     3  14   18
  Metal         0         0    4   229   3    9
  Pop           1         0    1     3 376    6
  Rock          1         1   17     7  14  521


Misclassification Rate =  0.0852


It's a strong start at only 8.5% missclassification, but with modification we should be able to bring that down. Like before, we will start by finding the optimal number of neighbors.

```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=2)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       390         3   18     7   6    6
  Classical     6       871  129     3   6    8
  Jazz          6        28  619     5  12   28
  Metal         0         0    2   224   3    8
  Pop           2         1    3     2 371    7
  Rock          0         1   24    12  17  505


Misclassification Rate =  0.106


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=3)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       389         1   14     8   5    6
  Classical    10       879  128     3   6    9
  Jazz          5        23  633     4  13   24
  Metal         0         0    1   230   4    5
  Pop           0         0    3     3 369    6
  Rock          0         1   16     5  18  512


Misclassification Rate =  0.0963


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=4)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       386         2   13     8   6   10
  Classical    13       878  149     4   5   11
  Jazz          5        24  607     3  21   25
  Metal         0         0    4   228   3    8
  Pop           0         0    4     2 365    7
  Rock          0         0   18     8  15  501


Misclassification Rate =  0.11


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=5)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       385         2   10     8   5    9
  Classical    12       883  151     6   6   10
  Jazz          5        19  611     3  13   22
  Metal         0         0    3   226   6    2
  Pop           2         0    4     3 368    7
  Rock          0         0   16     7  17  512


Misclassification Rate =  0.104


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=7)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       380         3   14     9   6   12
  Classical    16       881  165     5   8   14
  Jazz          7        20  592     3  15   26
  Metal         0         0    4   227   4    6
  Pop           1         0    3     2 359    7
  Rock          0         0   17     7  23  497


Misclassification Rate =  0.119


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=10)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       377         4   15    11   4   13
  Classical    18       877  190     6   7   15
  Jazz          9        23  567     2  16   27
  Metal         0         0    2   223   5    9
  Pop           0         0    3     2 362    8
  Rock          0         0   18     9  21  490


Misclassification Rate =  0.131


Clearly, the results are going no where with more neighbors. Its highly likely that keeping kn at 1 is going to produce the best results. Next, we will try to optimize weighting for our predictive model.


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 1)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       393         1   15     7   4    2
  Classical     5       872  100     4   4    6
  Jazz          4        30  656     2  14   18
  Metal         0         0    4   229   3    9
  Pop           1         0    1     3 376    6
  Rock          1         1   17     7  13  521


Misclassification Rate =  0.0858


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 2)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       392         1   11     7   3    2
  Classical     5       870   94     4   4    6
  Jazz          4        30  649     2  12   18
  Metal         0         0    4   229   3    9
  Pop           1         0    1     2 374    6
  Rock          1         1   17     7  13  521


Misclassification Rate =  0.0894


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 3)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       391         1   10     6   3    2
  Classical     5       864   87     3   3    6
  Jazz          4        30  621     0  12   16
  Metal         0         0    2   229   3    9
  Pop           1         0    1     2 373    5
  Rock          1         1   16     7  13  521


Misclassification Rate =  0.1


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 4)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       384         1    8     5   2    2
  Classical     3       847   74     3   0    6
  Jazz          3        23  589     0   9   14
  Metal         0         0    2   227   3    8
  Pop           1         0    1     2 363    3
  Rock          1         1   16     7  13  520


Misclassification Rate =  0.121


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 5)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       379         1    6     4   2    1
  Classical     2       809   47     2   0    6
  Jazz          3        18  544     0   8   12
  Metal         0         0    1   226   3    8
  Pop           0         0    0     2 336    3
  Rock          1         1   15     6  12  518


Misclassification Rate =  0.156


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 6)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       360         1    5     4   2    1
  Classical     2       774   37     0   0    5
  Jazz          3        16  499     0   5   11
  Metal         0         0    0   225   3    8
  Pop           0         0    0     2 303    2
  Rock          1         1   14     6   8  508


Misclassification Rate =  0.199


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 7)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       347         0    1     3   1    1
  Classical     2       727   28     0   0    4
  Jazz          1        15  450     0   3    8
  Metal         0         0    0   221   3    8
  Pop           0         0    0     2 260    2
  Rock          0         1    9     5   3  494


Misclassification Rate =  0.25


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 8)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       330         0    1     2   1    0
  Classical     0       665   19     0   0    3
  Jazz          1        12  393     0   3    7
  Metal         0         0    0   218   3    8
  Pop           0         0    0     2 226    0
  Rock          0         0    8     5   1  485


Misclassification Rate =  0.305


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 9)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       308         0    0     2   1    0
  Classical     0       615   11     0   0    1
  Jazz          0         7  333     0   2    6
  Metal         0         0    0   214   2    5
  Pop           0         0    0     0 199    0
  Rock          0         0    8     5   1  473


Misclassification Rate =  0.357


```{r}
# g.sknn = sknn(GENRE~.,data=gtrain,kn=1, gamma = 10)
# g.yhat = predict(g.sknn,newdata=gval)
# 
# misclass(g.yhat$class,gval$GENRE)
```

Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       284         0    0     1   0    0
  Classical     0       548    9     0   0    1
  Jazz          0         5  283     0   2    4
  Metal         0         0    0   209   0    5
  Pop           0         0    0     0 170    0
  Rock          0         0    7     3   1  461


Misclassification Rate =  0.413


While a few weighting schemes did come close, most still were not quite able to beat the more simpl model. Although gamma 1 was off by only 0.6%. Either way, it seems as though weighting will likely not be how we fully optimize our model, but it was worth our consideration. Perhaps utilizing some kind of distance metric in conjunction with a smoother will prove more bountiful.

```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 1)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       394         0    8     5   3    1
  Classical     6       879  116     4   3    8
  Jazz          3        25  659     0  16   15
  Metal         0         0    1   232   2    4
  Pop           1         0    1     2 378    6
  Rock          0         0   10    10  13  528


Misclassification Rate =  0.0789



It appears that a smallier distance paramter performs the best, getting our missclassification rate down to about 7.89%

Looking further at that model:

```{r}
# summary(g.knnD)

```


This model seems pretty solid using nominal, although we should try some greater combinations of distances and kernels to see what kind of improvement we might find. 


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 2)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       393         1   15     7   5    1
  Classical     6       872  102     4   3    7
  Jazz          4        30  657     3  14   18
  Metal         0         0    3   229   3    9
  Pop           0         0    1     3 375    6
  Rock          1         1   17     7  15  521


Misclassification Rate =  0.0858


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 3)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       384         5   19     8   4    2
  Classical     8       869  117     4   5    9
  Jazz          7        29  634     4  11   20
  Metal         1         0    2   225   3    9
  Pop           3         0    3     1 380    4
  Rock          1         1   20    11  12  518


Misclassification Rate =  0.0969


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 4)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       383         8   19    10   4    5
  Classical    10       862  132     3   7   11
  Jazz          8        33  618     5  13   24
  Metal         1         0    2   222   3   10
  Pop           1         1    4     2 376    6
  Rock          1         0   20    11  12  506


Misclassification Rate =  0.11



```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 5)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       382        11   25    10   3    4
  Classical    10       852  135     5   7   11
  Jazz          9        39  602     4  17   30
  Metal         1         0    2   217   3   12
  Pop           1         2    8     2 372    8
  Rock          1         0   23    15  13  497


Misclassification Rate =  0.123


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 6)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       380        12   24    12   6    6
  Classical    11       852  148     5   8   12
  Jazz         10        38  591     4  16   33
  Metal         1         0    2   217   3   13
  Pop           1         2    9     2 367   10
  Rock          1         0   21    13  15  488


Misclassification Rate =  0.131

```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 7)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       376        11   29    11   6    6
  Classical    15       848  159     4  10   14
  Jazz         11        40  579     5  16   37
  Metal         0         0    0   214   3   16
  Pop           1         4    6     4 361    9
  Rock          1         1   22    15  19  480


Misclassification Rate =  0.143


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 8)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues     cv        13   27    13   7    6
  Classical    14       843  161     5  10   13
  Jazz         11        42  574     4  12   35
  Metal         1         0    0   208   3   17
  Pop           3         5   11     4 365   11
  Rock          1         1   22    19  18  480


Misclassification Rate =  0.147


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 9)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       369        14   27    13   9    6
  Classical    17       839  161     5  10   13
  Jazz         12        45  572     4  13   35
  Metal         1         0    0   206   3   16
  Pop           3         5   12     5 362   12
  Rock          2         1   23    20  18  480


Misclassification Rate =  0.152


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 10)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       367        14   30    13  10    7
  Classical    16       838  163     5  10   14
  Jazz         14        46  568     4  14   36
  Metal         1         0    0   207   3   17
  Pop           3         5   12     5 360   12
  Rock          3         1   22    19  18  476


Misclassification Rate =  0.155


```{r}
# g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular",  "biweight"), distance = 11)
# yhatDg = predict(g.knnD,newdata=gval)
# 
# misclass(yhatDg,gval$GENRE)
```
Table of Misclassification
(row = predicted, col = actual)
           y
fit         Blues Classical Jazz Metal Pop Rock
  Blues       365        13   32    14  11    7
  Classical    16       839  165     5  10   14
  Jazz         16        45  563     4  14   39
  Metal         1         0    1   207   3   16
  Pop           3         5   12     4 359   13
  Rock          3         2   22    19  18  473


Misclassification Rate =  0.158


None of these models improve off of the simplest one in terms of distance, although kn 3 was very similar, so it may be worth trying that as well. Leaving our lowest distance model to be the best.

As one final thing we can try, we can use the tune function from e1071 to try and have it search for optimal paramters.


```{r}

#g.tune = tune.knn(x = gtrain[,-1], y = gtrain[,1],k = 1:10, tunecontrol = tune.control(sampling = "boot") )

#g.tune$best.performance

```

[1] 0.1006017

```{r}
#summary(g.tune)
```

k
<int>
error
<dbl>
dispersion
<dbl>
1	0.1006017	0.008796545		
2	0.1259869	0.005100039		
3	0.1319405	0.009213037		
4	0.1370575	0.006859476		
5	0.1389167	0.006680368		
6	0.1423835	0.009331596		
7	0.1437095	0.008892682		
8	0.1460595	0.009197514		
9	0.1468114	0.009646099		
10	0.1504351	0.008092739	

Trying out other cv functions.


```{r}

#g.tune2 = tune.knn(x = gtrain[,-1], y = gtrain[,1],k = 1:10, tunecontrol = tune.control(sampling = "cross") )

#g.tune2$best.performance

```

[1] 0.07588157

```{r eval=FALSE, include=FALSE}
#summary(g.tune2)
```

k
<int>
error
<dbl>
dispersion
<dbl>
1	0.07588157	0.009671225		
2	0.09537771	0.011274021		
3	0.09957562	0.012750349		
4	0.10706785	0.015673815		
5	0.11396666	0.018521511		
6	0.12115948	0.017660852		
7	0.12836130	0.016476175		
8	0.13136340	0.015945581		
9	0.13436191	0.014353106		
10	0.13796012	0.015357159	


While this does support that the a kn of 1 is optimal, it still is not able to produce a fundamentally different person.

Still our best model remains as a simple nominal, distance 1, kn 1 model.

```{r}
#g.knnD = train.kknn(GENRE~.,data=gtrain,kn=1, kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "biweight","triweight","gaussian","cos","rank"), distance = 1)
```

We will also use Monte Carlo on the full original training set.


```{r}
gtrain2 = read.csv("GenreTrain.csv")

gtrain.X2 = scale(gtrain2[,-192])

gtrain2 = data.frame(GENRE=gtrain2$GENRE,gtrain.X2)

results = kknn.sscv(SATimage,B=100,kmax=1, distance = 1, p = 0.33, kernel = "biweight")

summary(results)

```

With 100 passes, we seem to hover at a rate around 10%, which is a little worse then what we saw on the validation set, but not anything out of the realm of possibility.



Based on all of the testing done and our model building procedure, we reccomend a fairly simple k-nn model that considers only 1 other nearest neighbor, uses are Minkowski distance paramter of 1, and a biweight (beta(3,3)) kernel. While a simple model may seem odd, given the diverse nature of audio signals associated with different genres as well as the fact that certain elements are guranteed to be present in all of them, a model that focuses on just a few key aspects could be expected to perform much better than one would otherwise expect.

## Submitting Predictions

Using the k-NN classification procedure we chose above, we are going to use our model to predict music genre on the test data set.


```{r}

# g.knnD = train.kknn(GENRE~.,data=gtrain2,kn=1, kernel="biweight", distance = 1)

# final_pred = predict(g.knnD,newdata=gtest)

#write.csv(final_pred, file = "k-NN_audio_preidctions_Andrews_Weiczorek.csv")


```


# TYPE OF OIL BASED FATTY ACID CONTENT

## Data

The file <b>Oils.csv</b> contains fatty acid content readings for samples of seven types of oils (pumpkin, sunflower, peanut, olive, soybean, rapeseed, and corn).  The fatty acids measured are: Palmitic, Stearic, Oleic, Linoleic, Linolenic, Eicosanoic, and Eicosenoic.  

```{r}
setwd(getwd())
Oils =read.csv("Oils.csv")
summary(Oils)
```

## Data Preparation

### Train/Validation Split

It wouldn't be wise to split the data into train and validation sets as we only have a total of 96 observations in the original dataset. The decision making factor for not splitting the data is
the amount of observations representing each class. There is 7 classes of Type categorical variable. Splitting the dataset with stratifying the Type variable could be a potential problem-solver as it would keep the same proportion of observations belonging to each class. However, as we can see (below) the low level of counts of each class below, it is not doable to make each cateogry representative of the data if we attempted a train/validation set split.

```{r}
summary(Oils$Type)
```

### Standardizing 

All our predicotrs are numeric variables. Their scales differ a little bit, and as we are going to be using k-nearest neighbors to classify our observations, we will be working with straight line distance metrics - so we are going to standardize our data (mean=0, stdev=1). It is crucial when using k-nearest neighbors approach.

```{r}
Oils[,2:8] = scale(Oils[,2:8])
set.seed(888)
```

## Model building

We will be using k-NN and Na√Øve Bayes to classify the training data. 

### Unweighted kNN

Let's start with a simple, unweighted k-NN classification model. For this, we will use sknn() function from the klaR library.

Gamma parameter is a tunning parameter. Gamma = 0 gives us the unweighted model. As gamma increases, the more down-weighted we get, with weights dropping very fast.

```{r}
#Nearest Neighbors (k=1)
oils.sknn = sknn(Type~., data = Oils, kn=1)
```

```{r}
ypred = predict(oils.sknn)
```

predict() function returns two attributes: posterior and class. Posterior is a predicted probability of the observation being in each class; class gives us a predicted class.

Let's look at the missclassfication rate using the following function:
```{r}
missclass = function(fit,y) {
temp <- table(fit,y)
cat("Table of Misclassification\n")
cat("(row = predicted, col = actual)\n")
print(temp)
cat("\n\n")
numcor <- sum(diag(temp))
numinc <- length(y) - numcor
mcr <- numinc/length(y)
cat(paste("Misclassification Rate = ",format(mcr,digits=3)))
cat("\n")
}
```


```{r}
#Nearest Neighbors (k=1)
misclass(ypred$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=2)
oils.sknn = sknn(Type~., data = Oils, kn=2)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=3)
oils.sknn = sknn(Type~., data = Oils, kn=3)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=4)
oils.sknn = sknn(Type~., data = Oils, kn=4)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=5)
oils.sknn = sknn(Type~., data = Oils, kn=5)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=6)
oils.sknn = sknn(Type~., data = Oils, kn=6)
misclass(predict(oils.sknn)$class,Oils$Type)
```

The model with k=1 nearest neighbor is performing the best. However, when we use 1-nearest neighbor classifier, there is a high likelihood we are encountering overfitting of our data. Estimating the probability based on a single sample (first closest neighbor) is sensitive to noise, missclassification, When we use larger k values, we increase the robustness of our model, as then our decision surface gets smoother. However, for larger k values, our missclassificaiton rate was increasing. 

k=2 and k=3 were having the same performance. We are going to keep kn=3 in our model.
Let's implement weighing into our model and see if that can improve it. We are not going to be considering kn=1 from now on.

### Weighted kNN

```{r}
#Nearest Neighbors (k=2, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma = 1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=3, gamma=2)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma=2)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=4, gamma=3)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma = 3)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=5, gamma=4)
oils.sknn = sknn(Type~., data = Oils, kn=3, gamma=4)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=4, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=4, gamma=1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=5, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=5, gamma=1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=6, gamma=1)
oils.sknn = sknn(Type~., data = Oils, kn=6, gamma=1)
misclass(predict(oils.sknn)$class,Oils$Type)
```

```{r}
#Nearest Neighbors (k=6, gamma=2)
oils.sknn = sknn(Type~., data = Oils, kn=6, gamma=2)
misclass(predict(oils.sknn)$class,Oils$Type)
```

As seen above, using any combination of the number of nearest neighbors (kn = {1,6}) and gamma parameter (weighing), our missclassification rate is 0.

### Distance metric Tunning

Let's experiment with Minkowski distance metric.

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("optimal"), distance = 1)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("optimal"), distance = 2)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("optimal"), distance = 3)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

Putting more emphasis on Mikowski distance isn't chaning the missclassificaiton rate, perhaps modifying the kernel will make a difference.

### Kernel Tunning

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c("triangular"), distance = 1, gamma = 1)  
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3, kernel=c( "rectangular", "epanechnikov", "optimal", "triweight")) 
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

```{r}
oils.kknn = train.kknn(Type~.,data=Oils, kn=3,kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "triweight", "gaussian"), distance = 1)
misclass(predict(oils.kknn, newdata = Oils), Oils$Type)
```

Any of the weighted knn approaches give <b>NO missclassifications</b> on the training data. However we cannot know whether our model could be underfitting (having a high bias and low variance) or overfitting (low bias and high variance) as we are not predicting on a different training data set (validation set).

### Parameter Grid Search

As seen above, there is many many possible combinations to find optimal paramters. Let's use a very handy parameter grid search tool to have it find the best combination of tunning parameters for us.

This function tune() is in the package called "e1071"

First, we need to form our resposne vector 'y' and a data frame 'x' of our predictors.

```{r}
y = Oils[,1]
x = Oils[,-1]
```

Here, we have to be using knn() function to fit kNN classifier. Parameter 'l' specifies the minimum vote for the definite decision when there is a tie. If it's null, there is a doubt (Unkown "?").

```{r}
require(e1071)
tuned = tune.knn(x,y, k=2:15, tunecontrol = tune.control(sampling = "boot"), data = Oils)
summary(tuned)
```

Seems like lower k values are classifying our problem better. Also, this function is not so useful as intially thought - it does not search for optimal weighting gamma parameter or kernels. It is good, however, for a quick look at the performance of different number of nearest neighbors.

### Monte-Carlo Cross Validation for k-NN

As we mentioned above, Cross Validation is very important to measure accuracy of our model and see whether we have an optimal bias-variance trade-off.

We will randomly split the Oils data B times into training and validation sets using Monte-Carlo Cross Validation functions below.

```{r}
#For uing kknn()
kknn.cv = function(train,y=train[,1],B=25,p=.333,kmax=3,kernel="optimal",distance=2) {
  y = as.factor(y)
  data = data.frame(y=y,train[,-1])
  n = length(y)
  cv <- rep(0,B)
  leaveout = floor(n*p)
  for (i in 1:B) {
    sam <- sample(1:n,leaveout,replace=F)
    fit <- train.kknn(y~.,data=data[-sam,],kmax=kmax,kernel=kernel,distance=distance)
    ypred = predict(fit,newdata=data[sam,])
    tab <- table(y[sam],ypred)
    mc <- leaveout - sum(diag(tab))
    cv[i] <- mc/leaveout
  }
  cv
}

#For using sknn()
sknn.cv = function(train,y=train[,1],B=25,p=.333,k=3,gamma=0) {
y = as.factor(y)
data = data.frame(y,train)
n = length(y)
cv <- rep(0,B)
leaveout = floor(n*p)
for (i in 1:B) {
        sam <- sample(1:n,leaveout,replace=F)
        temp <- data[-sam,]
        fit <- sknn(y~.,data=temp,kn=k,gamma=gamma)
        pred = predict(fit,newdata=train[sam,])$class
        tab <- table(y[sam],pred)
        mc <- leaveout - sum(diag(tab))
        cv[i] <- mc/leaveout
        }
  cv
}


```

The data frame needs to have the response as the first variable for the functions above.

#### For sknn

train,y=train[,1],B=25,p=.333,k=3,gamma=0)
```{r}
set.seed(888)
results = sknn.cv(Oils,B=25,k=3,gamma=0)
summary(results)
```

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=3,gamma=1)
summary(results)
```

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=3,gamma=2)
summary(results)
```

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=5,gamma=2)
summary(results)
```

Best metrics for k=3 with gamma=2 (M=0.01032) and k=5 with gamma=2 (M=0.0129)

#### For kknn

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=15,kernel="optimal")
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=4,kernel="triangular")
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=10,kernel="triangular",distance=1) 
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=15,kmax=5,kernel="triangular",distance=1) 
summary(results)
```

```{r}
set.seed(888)
results = kknn.cv(Oils,B=10,kmax=10,kernel=c("triangular", "rectangular", "epanechnikov", "optimal", "triweight", "gaussian"),distance=1)
summary(results)
```

Metrics for sknn were of better performance.


### Naive Bayes

Which type of Oils has the highest posterior probability of being observed given the data?
fL = tunning parameter Laplace


#### Normal Distribution

Which of the fatty acids have the best discriminatory ability in classying which type of Oil it is? Let's look at two examples.

```{r warning=F}
boxqq(Palmitic~Type, data=Oils)
```

High levels are associated with the given type. For example, for Palmitic fatty acid, we can see it would most likley be associated with Type D. And vice-versa, if the Linoleic levels are low, there is a good chance, they are associated with Type D (as seen below).

For Palmitic, there is a tiny bit of right skewness for Type B and left skewness for Type A and D (with D seeming to have an outlier). 

```{r warning=F}
boxqq(Linoleic~Type, data=Oils)
```

For Linoleic fatty acid content, normality looks pretty okay across the seven Type categories (Type F looks a bit skewed right).


```{r warning=F}
boxqq(Stearic~Type, data=Oils)
```

For Stearic fatty acid content, we see that the Type A and B are right skewed.


```{r warning=F}
boxqq(Oleic~Type, data=Oils)
```

For Oleic fatty acid content, we see that the Types follow the normal QQ reference lines (Type D seems left skewed)

```{r warning=F}
boxqq(Linolenic~Type, data=Oils)
```

Here, for Linolenic, Type F seems to be right skewed.


```{r warning=FALSE}
boxqq(Eicosanoic~Type, data=Oils)
```

For Eicosanoic fatty acid content, we can see that for Type A, there is a huge outlier. Type B, E, and F are skewed left and Type D is skewed right.

```{r warning=F}
boxqq(Eicosenoic~Type, data=Oils)
```

Normality of Eicosenoic looks like being left skewed for Type F and right skewed for Type C.


We keep everything standardized (mean=0 and stddev=1)
```{r}
require(s20x)
set.seed(888)
oils.nb = naiveBayes(Type~., data = Oils, laplace = 1)
ypred = predict(oils.nb, newdata = Oils[,-1], type = "raw") # type="raw" is giving probability
head(ypred)
```

Giving the estimated probabiliy for each of the observation being each Type. We want to chose the one that is the largest. 

Observation from 1 to 6 given above (as rows). The first one gets classified as being of Type G, the second one gets classified as being of Type D, the third one gets classified as being of Type E, the fourth one gets classified as being of Type D, the fifth one gets classified as being of Type E, and the sixth one gets classified as being of Type F.

--The interpretation would work like that if there wasn't the beautiful and odd Type A column - it is classifying everything to its Type A with probabilites being 1...?

If you do not type in type="raw" in predict() we would get teh acutal class the predicates belong to. Let's look into the missclassification rate:

```{r}
ypred = predict(oils.nb, newdata = Oils[,-1]) #getting classes
misclass(ypred,Oils$Type)
```

We are getting the same missclassification rate (1.04%) at this point as we were getting with the unweighted kNN (k=3).

### Monte-Carlo Cross Validation for Naive Bayes
```{r}
nB.cv = function(X,y,B=25,p=.333,laplace=0) {
y = as.factor(y)
data = data.frame(y,X)
n = length(y)
cv <- rep(0,B)
leaveout = floor(n*p)
for (i in 1:B) {
        sam <- sample(1:n,leaveout,replace=F)
        temp <- data[-sam,]
        fit <- naiveBayes(y~.,data=temp,laplace=laplace)
        pred = predict(fit,newdata=X[sam,])
        tab <- table(y[sam],pred)
        mc <- leaveout - sum(diag(tab))
        cv[i] <- mc/leaveout
        }
  cv
}
```

Let's use the Monte-Carlo Cross-Validation, randomly splitting Oils data B times.

```{r}
set.seed(888)
X = Oils[,-1]
y = Oils[,1]
results = nB.cv(X, y, B=500, laplace = 1)
summary(results)
```

On average, we are getting about 8.6% missclassified cases.

### kNN final model:

```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=3,gamma=1)
summary(results)
```


```{r}
set.seed(888)
results = sknn.cv(Oils,B=500,k=5,gamma=2)
summary(results)
```

Missclassification rate (on average), 2.09% (k=3, gamma=1) and 1.97% (k=5, gamma=2)

### Naive Bayes final mode:

```{r}
set.seed(888)
X = Oils[,-1]
y = Oils[,1]
results = nB.cv(X, y, B=500, laplace = 1)
summary(results)
```

Missclassification rate (on average), 8.6%

Naive Bayes may be more suited for categorical predictors, not numeric ones (our Oils data has all numeric predictors).

k-NN is the winner (the data needs to have more observations to ensure quality of representation of each Type category).













