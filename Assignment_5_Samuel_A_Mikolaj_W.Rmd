---
title: "Assignment 5"
author: "Samuel A, Mikolaj W"
date: "March 6, 2020"
output: word_document
---

Problem 2 – CRYSTAL MELTING POINT DATA
In their 2005 paper “General Boiling Point Prediction Based on a Diverse Compound Data Set and Artificial Neural Networks”, Karthikeyan, Glen, and Bender examine methods for the prediction of melting points using a number of 2D and 3D descriptors that capture molecular physicochemical and other graph-based properties.  The melting point is a fundamental physicochemical property of a molecule that is controlled by both single-molecule properties and intermolecular interactions due to packing in the solid state.  Thus, it is difficult to predict, and previously only melting point models for clearly defined and smaller compound sets have been developed.   The data frame QSAR.melt contains data for 4401 compounds that can be used to develop a model for melting point.  There data are contained in the file QSAR Melting Points (subset).csv on the course website.

Goal:  Use PCR and PLS develop models and compare their ability to predict melting 
           point.  


Packages:
```{r setup, include=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(e1071)
require(nnet)
require(car)
require(ISLR)
require(MASS)
require(glmnet)
require(pls)
require(corrplot)
require(caret)
```

Data Preperation:
```{r}
knitr::opts_chunk$set(include = FALSE)
QSAR.melt = read.csv("Melting Point QSAR.csv")

#preprocess this data for 203 predictors
QSAR.bc = preProcess(QSAR.melt, method = "BoxCox")
QSAR.melt = predict(QSAR.bc, QSAR.melt)

set.seed(1)
QSAR.melt = QSAR.melt[,-1] # remove Case column which is an ID
train = sample(nrow(QSAR.melt),3900)
test = -(train)
X = QSAR.melt[,-1]   # grab all the predictors, Y = MTP is the 1st column
Xs = scale(X)	     # scale the predictors
QSAR = data.frame(MTP=QSAR.melt$MTP,Xs)
qsar.train = QSAR[train,]
qsar.test = QSAR[test,]

```



a)	 Develop an “optimal” PCR model.  Justify your choice using cross-validation. (10 pts.)

Now we will look at a corrplot as well and generate a correlation matrix.
```{r}
QSAR.cor = cor(x =QSAR.melt, use = "na.or.complete") #deal with missing values

corrplot(QSAR.cor, order = 'hclust', hclust.method = 'complete', tl.pos = 'n' )

detach(package:corrplot)
```

While the NA values do dampen the overall effectiveness, we still are able to seem some correlation bewteen the various variables, we should bode well for our model in terms of variable reduction.
Obviously due to our missing values we get uncalculatable correlations, shown as gray in the plot above.
We will now fit a pcr model with a bunch of components and use a validation plot to attempt to narrow the
number of components used down to a more reasonable number. We will use 40 to start as instructed
by the code provided.

```{r}
qsar.pcr = pcr(MTP~.,ncomp=40,validation="CV", scale = T, data=qsar.train)

summary(qsar.pcr)
```

Looking at CVs, we see that about 40 components appears to be optimal. Although for variability explained
in the melting point, that value first reaches 90 at about 28 components. This is promising, but to be throughout we wanted to look at a wider scale. This lead us to wideing our search to up to 100 components just to see what might be found.

```{r}
qsar.pcr = pcr(MTP~.,ncomp=100,validation="CV", scale = T, data=qsar.train)

summary(qsar.pcr)

```

Based on CV values here, we get ever increasing components used, upwards towards our limit, but the returns are ever diminishing. However, in terms of meaningful increases in variation exaplined, nothing past 67 components has a discernable impact, and anything 41 or above is greater then 95%, giving us a decent range of values to investigate further.

To get a better sense of these patterns overall, we also looked at a validation plot.


```{r}
validationplot(qsar.pcr, val.type = "RMSEP")
```

This plot lines up with our stated results quite nicely, with the late 60s being the last values to truly make a difference while a vast majority of the work is done by roughly 40 of them. Now that we have our candiate values, we can further use cross-validation on the test set to pick the best one.


```{r include=FALSE}
yact = qsar.test$MTP

ypred = predict(qsar.pcr, ncomp = 41, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.55985046894924"

ypred = predict(qsar.pcr, ncomp = 42, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.53711890597628"

ypred = predict(qsar.pcr, ncomp = 43, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.53827397067486"

ypred = predict(qsar.pcr, ncomp = 44, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.5303035536344"

ypred = predict(qsar.pcr, ncomp = 45, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.48699166773672"

ypred = predict(qsar.pcr, ncomp = 46, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.48611097491785"

ypred = predict(qsar.pcr, ncomp = 47, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.46567990229641"

ypred = predict(qsar.pcr, ncomp = 48, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.47194211820051"

ypred = predict(qsar.pcr, ncomp = 49, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.46930828577019"

ypred = predict(qsar.pcr, ncomp = 50, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.43156297639954"


ypred = predict(qsar.pcr, ncomp = 51, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.43135633338793"

ypred = predict(qsar.pcr, ncomp = 52, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.42057808785403"


ypred = predict(qsar.pcr, ncomp = 53, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.39756284309691"

ypred = predict(qsar.pcr, ncomp = 54, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.38667367545324"

ypred = predict(qsar.pcr, ncomp = 55, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.36172667707251"

ypred = predict(qsar.pcr, ncomp = 56, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.37699193673916"

ypred = predict(qsar.pcr, ncomp = 57, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.37368832848207"

ypred = predict(qsar.pcr, ncomp = 58, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.36394359175292"

ypred = predict(qsar.pcr, ncomp = 59, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.35095658191415"

ypred = predict(qsar.pcr, ncomp = 60, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.34722595702933"

ypred = predict(qsar.pcr, ncomp = 61, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.35327096257996"

ypred = predict(qsar.pcr, ncomp = 62, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.33764621264906"

ypred = predict(qsar.pcr, ncomp = 63, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.33320007487676"

ypred = predict(qsar.pcr, ncomp = 64, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.32901647673303"

ypred = predict(qsar.pcr, ncomp = 65, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.30334531059486"

ypred = predict(qsar.pcr, ncomp = 66, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.28778011423847"

ypred = predict(qsar.pcr, ncomp = 67, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.28117819589367"

ypred = predict(qsar.pcr, ncomp = 68, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.28287674977966"

ypred = predict(qsar.pcr, ncomp = 69, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.28639638441397"

ypred = predict(qsar.pcr, ncomp = 70, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.28703099462511"

ypred = predict(qsar.pcr, ncomp = 71, newdata = qsar.test)
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.27315740303121"

```

Using RMSEP as our metric, we can see that the sweet spot appears to be 66 principle components. While that is a lot, compared to the 203 variables we started with that is roughly a 67% reduction in the number or predictors while also dealing with any collineation that was present. Our optimal PCR model uses 66 components.


b)	 Develop an “optimal” PLS model.  Justify your choice using cross-validation.  (10 pts.)

```{r}
qsar.pls = plsr(MTP~.,ncomp=40,validation="CV", scale = TRUE, data=qsar.train)
summary(qsar.pls)

```

Looking at CVs, it seems that by the time we get to 30 components we begin to chase our tails as far as improvements go. This lines up with what we see with percent of variation explained, as we stop seeing any dramatic impacts past around 27 components. Much like with pcr, for the sake of being thorough, we also looked at 100 components, although with pcl especially it is doubtful that anything of greater impact will be found.

```{r}
qsar.pls = plsr(MTP~.,ncomp=100,validation="CV", scale = TRUE, data=qsar.train)
summary(qsar.pls)
```

The only added conclusion we can make now is that it would take about 53 components to explain 95% of the variation. However, the amount of small increments needed to get there could make one hesitant to use that many components.


Looking at a plot of some internalally generated RMSEPs can help us make a decision as well.
```{r}
plot(RMSEP(qsar.pls), legendpos = "topright")
```
Looking at this plot, we can affirm our understanding based off of the raw numbers, as we begin to start getting diminshing returns around the late 20 values, with anything past it not really gaining anything substantial. From here, we can now to some simple cross-validation on a test set to narrow done to an optimal value.


```{r include=FALSE}

ypred = predict(qsar.pls,ncomp=15,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.23322236871757"

ypred = predict(qsar.pls,ncomp=16,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.21110396320829"

ypred = predict(qsar.pls,ncomp=17,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.18492232023154"

ypred = predict(qsar.pls,ncomp=18,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.17286816385355"

ypred = predict(qsar.pls,ncomp=19,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.13469610335237"

ypred = predict(qsar.pls,ncomp=20,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.12842673233427"

ypred = predict(qsar.pls,ncomp=21,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.10055233123103"

ypred = predict(qsar.pls,ncomp=22,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.09748013448897"

ypred = predict(qsar.pls,ncomp=23,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.08947154781649"

ypred = predict(qsar.pls,ncomp=24,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.06296788926204"

ypred = predict(qsar.pls,ncomp=25,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.0223431390556"

ypred = predict(qsar.pls,ncomp=26,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.00714014366385"

ypred = predict(qsar.pls,ncomp=27,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.02572471482335"

ypred = predict(qsar.pls,ncomp=28,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.0278669763282"

ypred = predict(qsar.pls,ncomp=29,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

# "RMSEP: 6.02910360178234"

ypred = predict(qsar.pls,ncomp=30,newdata=qsar.test)
yact = qsar.test$MTP
paste("RMSEP:", sqrt(mean((ypred-yact)^2, na.rm = TRUE)))

#"RMSEP: 6.03459557696351"

```


While not immediatley clear from our plots or numeric measure, based on actual cross-validaion our best metric was returned by a value an amount of components as little as 16. This makes sense given PLS's nature to return less then pcr, but the ability to reduce the dimensionality by nearly 92% is quite impressive. 16 components will be the basis for our optimal model.


c)	 Use your “optimal” model for each method (PCR and PLS) to predict the melting point 
 of the test cases. Which modeling method performs the best when predicting the test 
 cases?  (10 pts.)

```{r include = FALSE}

ypred = predict(qsar.pcr,ncomp=66,newdata=qsar.test)
ytest = qsar.test$MTP

Rsq.pred = 1 - (sum((ypred-ytest)^2, na.rm = TRUE)/sum((ytest - mean(ytest)^2), na.rm = TRUE))
Rsq.pred
#0.4539924

sum(predict(qsar.pcr, ncomp = 66, newdata = qsar.test) - qsar.test$MTP, na.rm = TRUE)^2
#75529.7


```

```{r}
plot(ytest,ypred,xlab="Actual Test MTP",ylab="Predicted Test MTP")
```


```{r include=FALSE}

ypred = predict(qsar.pls,ncomp=16,newdata=qsar.test)
ytest = qsar.test$MTP

Rsq.pred = 1 - (sum((ypred-ytest)^2, na.rm = TRUE)/sum((ytest - mean(ytest, na.rm = TRUE))^2))
Rsq.pred
#0.4672278

sum(predict(qsar.pls,ncomp=16,newdata=qsar.test) - qsar.test$MTP, na.rm = TRUE)^2
#73332.51
```

```{r}
plot(ytest,ypred,xlab="Actual Test MTP",ylab="Predicted Test MTP")
```

Based on the sums of sqaures of prediction error, we can see that the PLS model performed slightly better, which makes sense given that it had much more variable reduction  as well as more heavily focusing on the prediction of y then the organization of the x'compared to the pcr. However, the RMSE was ever so slightly in favor of the pcr. While the difference isn't major, as shown by the plots, the pls is likely better given its level of reduction. However there is likely still something to be gained from looking at the relevant loadings of the pcr model, as it may help organize the variables which could be useful for unsupervised applications.


d)	 Use the pcr.cv and pls.cv functions to assess the predictive performance of these two  
 methods.  Summarize the results.  (10 pts.)

Monte-carlo cross-validation functions accounting for missing values.
```{r}
pls.cv = function(X,y,ncomp=2,p=.667,B=100) {
	n = length(y)
	X = scale(X)
data = data.frame(X,y)
cv <- rep(0,B)
for (i in 1:B) {
ss <- floor(n*p)
sam <- sample(1:n,ss,replace=F)
fit2 <- plsr(y~.,ncomp=ncomp,data=data[sam,])
ynew <- predict(fit2,ncomp=ncomp,newdata=data[-sam,])
cv[i] <- sqrt(mean((y[-sam]-ynew)^2,na.rm=T))
}
cv
}

pcr.cv = function(X,y,ncomp=2,p=.667,B=100) {
	n = length(y)
	X = scale(X)
data = data.frame(X,y)
cv <- rep(0,B)
for (i in 1:B) {
ss <- floor(n*p)
sam <- sample(1:n,ss,replace=F)
fit2 <- pcr(y~.,ncomp=ncomp,data=data[sam,])
ynew <- predict(fit2,ncomp=ncomp,newdata=data[-sam,])
cv[i] <- sqrt(mean((y[-sam]-ynew)^2,na.rm=T))
}
cv
}


mean(pcr.cv(X = X, y = QSAR.melt$MTP, ncomp = 66, p = 0.667, B = 100))

#6.255307

mean(pls.cv(X = X, y = QSAR.melt$MTP, ncomp = 16, p = 0.667, B = 100))

#6.077052

```

Echoing the results of our more rudimentary cross-validation in previous problems, the PLS performs slightyl better over the long term with much more reductive power. However, after 100 samplings, we now can now more confidently state that the pls should perform better as a model on the whole. Between their handling of collinearity and reduction of variables, both methods though certainly earn their stripes and should be kept in mind as an alternative to other methods to handle certain problems.





