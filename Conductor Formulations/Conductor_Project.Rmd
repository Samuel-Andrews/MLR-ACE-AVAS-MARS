---
title: "Conductor_Project"
author: "Samuel A, Mikolaj W"
date: "April 3, 2020"
output: word_document
---


Preperation Work and packages
```{r include=FALSE}

setwd("C:/Users/qs5834mm/Desktop/DSCI 425/Git_R/Assignment_2/MLR-ACE-AVAS-MARS/Conductor Formulations")
load("C:/Users/qs5834mm/Desktop/DSCI 425/R/mult.Rdata")
load("C:/Users/qs5834mm/Desktop/DSCI 425/R/Regression.Rdata")

cond_com =  read.csv("Conductors (combined).csv")
cond_train = read.csv("Conductors (train).csv")
cond_test = read.csv("Conductors (test).csv")

require(dplyr)
require(ggplot2)
require(tidyr)
require(stringr)
library(car)
library(Ecfun)



View(cond_com)



```

Format the response in the training set to ln(y + 1).
```{r}
cond_train2 = cond_train

cond_train %>%
  mutate(formation_energy_ev_natom_log_y_plus_1 = log(formation_energy_ev_natom + 1), 
         bandgap_energy_ev_log_y_plus_1 = log(bandgap_energy_ev + 1)) -> cond_train2

drop.cols <- c('formation_energy_ev_natom', 'bandgap_energy_ev')

cond_train2 %>%
  select(-all_of(drop.cols)) -> cond_train2
  

```

Starting out, well try and see what transformations would be optimal for existing variables. There are only 11 in total be defualt, so going through each on an individual basis is more then doable. We also will follow a doctrine of optimization being greater then explainability for this approach, as one of our chief goals is predictions quality of predictions.


```{r}

cond_train2_trans = cond_train2

#checks for optimal transformations
myBC = function(y)
{
  BCtran(y)
  results = powerTransform(y)
  summary(results)
}


```

Space group itself is an interesting variable, as while it seems ordinal at first, space groups themselves are a bit complex and a higher group number is not a consitant difference. This will be accounted for later.

Starting with Total Atoms
```{r}

Statplot(cond_train2$number_of_total_atoms)
myBC(cond_train2$number_of_total_atoms)

cond_train2_trans$number_of_total_atoms = bcPower(cond_train2$number_of_total_atoms, 1.6)

Statplot(cond_train2_trans$number_of_total_atoms)

```


These next three variables measure the makeup of the elements Galluim, Aluminum, and Indium as percentages. All three are of course related in that regard, and certain makeups may include some
or all of each of these. This may make it difficult to view each as an individual in terms of model
utilization, but there are a few things we can try. First, we can go through and simply transform each
variable.


Percent_atom_al is next
```{r}

Statplot(cond_train2$percent_atom_al)



```
Since we are workiing with percentage data, we thought it would be best to use a logit trasnformation here.

```{r}
cond_train2_trans$percent_atom_al = logit(cond_train2$percent_atom_al)

Statplot(cond_train2_trans$percent_atom_al)
```
The improvement is noticable, although the zeros still tend to have a negative impact.

Next was percent_atom_ga

```{r}
Statplot(cond_train2$percent_atom_ga)
```
This variale similarly was best handled with logit.

```{r}
cond_train2_trans$percent_atom_ga = logit(cond_train2$percent_atom_ga)

Statplot(cond_train2_trans$percent_atom_ga)
```

We still have a problem with zeros, although in every other regard we have improved.


percent_atom_in is the final variable in this series, and likewise was handled similarly.

```{r}
Statplot(cond_train2$percent_atom_in)
```


```{r}
cond_train2_trans$percent_atom_in = logit(cond_train2$percent_atom_in)

Statplot(cond_train2_trans$percent_atom_in)
```



Next, we have a series of lattice vector variables that should be simple enough to box cox transform.

```{r}
Statplot(cond_train2$lattice_vector_1_ang)
```

Our myBC function tends to indicate using a power of 0.6, however, better results through testing were found in the for of a 4th root transformation. While the result is still not ideal, the improvement is still massive and should not be discounted. This may prove to be the begining of a pattern in terms of how the lattive vectors all hold.

```{r}
myBC(cond_train2$lattice_vector_1_ang)

cond_train2_trans$lattice_vector_1_ang = bcPower(cond_train2$lattice_vector_1_ang, 0.25)

Statplot(cond_train2_trans$lattice_vector_1_ang)
```
The next vector angle has similar shortcomings, but uses a different solution.

```{r}
Statplot(cond_train2$lattice_vector_2_ang)
```

This time, the bcPower function was more on target, with the power transformation of 0.7 being the best. Unfotunatley, the best in this case is not a very desireable variable structure, but once again still is an improvement.
```{r}
myBC(cond_train2$lattice_vector_2_ang)

cond_train2_trans$lattice_vector_2_ang = bcPower(cond_train2$lattice_vector_2_ang, 0.7)

Statplot(cond_train2_trans$lattice_vector_2_ang)
```
For the last vector angle, we have a similar story.

```{r}
Statplot(cond_train2$lattice_vector_3_ang)
```
The box cox gave the best result again with a negative 5th root, although much like the former our results still have shortcomings despite the imrpovements.
```{r}
myBC(cond_train2$lattice_vector_3_ang)

cond_train2_trans$lattice_vector_3_ang = bcPower(cond_train2$lattice_vector_3_ang, -0.25)

Statplot(cond_train2_trans$lattice_vector_3_ang)
```
The next three predictors are angle degrees, which all seem to hover around 90 degrees with occasional variation coming off of each. This will hopefull scale easily as a result.

Starting with lattice angle alpha degree

```{r}
Statplot(cond_train2$lattice_angle_alpha_degree)
```

We can see some unique problems due to the nature of degree mseasures here, so we will try converting to radians in order to assist in this.

```{r}
radians_to_degree <- function(rad) {
  (rad * 180) / (pi)
}

degree_to_rad <- function(deg) {
  (deg * pi) / (180)
}


cond_train2_trans$lattice_angle_alpha_degree = degree_to_rad(cond_train2$lattice_angle_alpha_degree)

Statplot(cond_train2_trans$lattice_angle_alpha_degree)
```
This should hopefully allow for easier transformation.



```{r}
myBC(cond_train2_trans$lattice_angle_alpha_degree)

cond_train2_trans$lattice_angle_alpha_degree = bcPower(cond_train2_trans$lattice_angle_alpha_degree, -11.5 )

Statplot(cond_train2_trans$lattice_angle_alpha_degree)
```

However, while it allows us to at least transform the data, it still does not allow us to truly fix it. We can box cox transform it with an abhorrent -11.5, but the massive amount of observations that close in around 90 simply leave us with odd results. 

We see similar issues with the next to angle variables

```{r}
cond_train2_trans$lattice_angle_beta_degree = degree_to_rad(cond_train2$lattice_angle_beta_degree)

Statplot(cond_train2_trans$lattice_angle_beta_degree)
```

```{r}
myBC(cond_train2_trans$lattice_angle_beta_degree)

cond_train2_trans$lattice_angle_beta_degree = bcPower(cond_train2_trans$lattice_angle_beta_degree, -15.5 )

Statplot(cond_train2_trans$lattice_angle_beta_degree)
```

```{r}

cond_train2_trans$lattice_angle_gamma_degree = degree_to_rad(cond_train2$lattice_angle_gamma_degree)

Statplot(cond_train2_trans$lattice_angle_gamma_degree)
```

```{r}
myBC(cond_train2_trans$lattice_angle_gamma_degree)

cond_train2_trans$lattice_angle_gamma_degree = bcPower(cond_train2_trans$lattice_angle_gamma_degree, 2.2)

Statplot(cond_train2_trans$lattice_angle_gamma_degree)
```

Ultimatley, what this process shows us is that in their current states, even transformed, many of these variables are not likely to be of great use to us. However, with a little enginuity we might be able to stretch these a little farther.


Going back to Space groups, we can create categorical variables for each group as a way to focus in on that variables, as only 6 of the possible 230 existing space groups are actually used.


```{r}
require(sjmisc)

cond_train2_trans %>% 
  to_dummy(spacegroup, suffix = "label" ) %>% 
  bind_cols(cond_train2_trans) %>% 
  select(ID, spacegroup, everything()) -> cond_train3

cond_train3 %>%
  rename(Spacegroup_12 = V1, Spacegroup_33 = V2, Spacegroup_167 = V3, Spacegroup_194 = V4, Spacegroup_206 = V5, Spacegroup_227 = V6) -> cond_train3


```



Number of total atoms, while numeric, only has 6 unique values across 1,567 rows. While the consistancy is nice, it does make our ability to transform it into a normal scale fairly difficuly as seen pervisouly. To circumvent this problem, we have elected to turn it into categorical data as well, as we feel that the benefit of being able to focus in on a category of atoms outweights the loss in ordinal power. However, certain models still will be fed one or the other based on their preference.

```{r}
cond_train3 %>% 
  to_dummy(number_of_total_atoms, suffix = "label" ) %>% 
  bind_cols(cond_train3) %>% 
  select(ID, number_of_total_atoms, everything()) -> cond_train3

  cond_train3 %>%
  rename(  atoms_10 = V1, atoms_20 = V2, atoms_30 = V3, atoms_40 = V4, atoms_60 = V5, atoms_80 = V6) -> cond_train3



```


We also want to make dummy varaibles based on whether or not certain elements are present within a potential superconductor, as this can help mitigate colineation problems in situations where these percentage varaibles would otherwise be related, and additionally they may prove useful on their own.


```{r}
cond_train3 %>%
  mutate(Aluminum = ifelse(percent_atom_al > 0, 1, 0), Gallium = ifelse(percent_atom_ga > 0, 1, 0),  Indium = ifelse(percent_atom_in > 0, 1, 0)) -> cond_train3


```

Now, we will go back and do all of this to the test set

```{r}
cond_test_ready = cond_teat

cond_test_ready$number_of_total_atoms = bcPower(cond_teat$number_of_total_atoms, 1.6)

cond_test_ready$percent_atom_al = logit(cond_teat$percent_atom_al)

cond_test_ready$percent_atom_ga = logit(cond_teat$percent_atom_ga)

cond_test_ready$percent_atom_in = logit(cond_teat$percent_atom_in)

cond_test_ready$lattice_vector_1_ang = bcPower(cond_teat$lattice_vector_1_ang, 0.25)

cond_test_ready$lattice_vector_2_ang = bcPower(cond_teat$lattice_vector_2_ang, 0.7)

cond_test_ready$lattice_vector_3_ang = bcPower(cond_teat$lattice_vector_3_ang, -0.25)

cond_test_ready$lattice_angle_alpha_degree = degree_to_rad(cond_teat$lattice_angle_alpha_degree)

cond_test_ready$lattice_angle_beta_degree = degree_to_rad(cond_teat$lattice_angle_beta_degree)

cond_test_ready$lattice_angle_gamma_degree = degree_to_rad(cond_teat$lattice_angle_gamma_degree)

cond_test_ready$lattice_angle_alpha_degree = bcPower(cond_teat$lattice_angle_alpha_degree, -11.5 )

cond_test_ready$lattice_angle_beta_degree = bcPower(cond_teat$lattice_angle_beta_degree, -15.5 )

cond_test_ready$lattice_angle_gamma_degree = bcPower(cond_teat$lattice_angle_gamma_degree, 2.2)

#spacegroup
cond_test_ready %>% 
  to_dummy(spacegroup, suffix = "label" ) %>% 
  bind_cols(cond_test_ready) %>% 
  select(ID, spacegroup, everything()) -> cond_test_ready

cond_test_ready %>%
  rename(Spacegroup_12 = V1, Spacegroup_33 = V2, Spacegroup_167 = V3, Spacegroup_194 = V4, Spacegroup_206 = V5, Spacegroup_227 = V6) -> cond_test_ready


#total atoms
cond_test_ready %>% 
  to_dummy(number_of_total_atoms, suffix = "label" ) %>% 
  bind_cols(cond_test_ready) %>% 
  select(ID, number_of_total_atoms, everything()) -> cond_test_ready

  cond_test_ready %>%
  rename(  atoms_10 = V1, atoms_20 = V2, atoms_30 = V3, atoms_40 = V4, atoms_60 = V5, atoms_80 = V6) -> cond_test_ready

  
  cond_test_ready %>%
  mutate(Aluminum = ifelse(percent_atom_al > 0, 1, 0), Gallium = ifelse(percent_atom_ga > 0, 1, 0),  Indium = ifelse(percent_atom_in > 0, 1, 0)) -> cond_test_ready

  
  
  
cond_test_ready %>%
  rename( formation_energy_ev_natom_log_y_plus_1 = formation_energy_ev_natom, bandgap_energy_ev_log_y_plus_1 = bandgap_energy_ev) -> cond_test_ready

```



Writing both .csv files for future use

```{r}

write.csv(cond_train3, "cond_train_new_variables.csv", row.names = FALSE)

write.csv(cond_test_ready, "cond_test_new_variables.csv", row.names = FALSE)

```

# Exploring structure within predictors with PCA

Load data and prep:
```{r}
###### Load and prepare data ######
#Load data:
{#setwd("~/OneDrive - MNSCU/myGithub/Supervised_Learning/Multiple_Linear_Regression/MachineLearning-SupervisedLearning/Conductor Formulations")
  #Train set
  cond_train = read.csv("Conductors (train).csv")
  #Test set
  cond_test = read.csv("Conductors (test).csv")
  
  ### New training sets
  cond_train_new_vars = read.csv("cond_train_new_variables.csv")
  #No_box_cox
  #angles measures are still in degrees; no transformaitons on predictors
  cond_train_new_vars_no_box_cox = read.csv("cond_train_new_variables_no_box_cox.csv")
  
  ####### New Variables #######
  
  #Resposne vectors 
  y_train_form_eng_trans = cond_train_new_vars$formation_energy_ev_natom_log_y_plus_1
  y_train_bandgap_eng_trans = cond_train_new_vars$bandgap_energy_ev_log_y_plus_1
  
  #Resposne vectors for no box_cox
  #y_train_form_eng_trans = cond_train_new_vars_no_box_cox$formation_energy_ev_natom_log_y_plus_1
  #y_train_bandgap_eng_trans = cond_train_new_vars_no_box_cox$bandgap_energy_ev_log_y_plus_1
  
  #Df of all x's
  x_train = cond_train_new_vars[,-c(1,25,26)]
  #x_train =cond_train_new_vars_no_box_cox[,-c(1,2,26,27)]
}
#Rearrange the data for two models: Model 1: formation energy as response; Model 2: bandgap energy as response
{#detach("package:Ecfun", unload = TRUE)
  cond_train_new_vars %>%
    select(25,2:24,27:29) -> train_model1
  
  cond_train_new_vars%>%
    select(26, 2:24,27:29) -> train_model2
} 
#Rearrange the data for two models for no box_cox
{
  #detach("package:Ecfun", unload = TRUE)
  cond_train_new_vars_no_box_cox %>%
    select(26,3:25,28:30) -> train_model1_no_box_cox
  
  cond_train_new_vars_no_box_cox%>%
    select(27, 3:25,28:30) -> train_model2_no_box_cox
} 
```

We are going to be working with 'x_train' data frame. It only includes our final (added and transformed) independent variables.
```{r}
x_train.mat = as.matrix(x_train)
```

Let's first explore the correlation of our predictors by building a correlation plot:

```{r fig.height=7, fig.width=7}
{corr.x_train = cor(x_train.mat)
#jpeg("corr.plot_predictors.png", width = 500, height = 500)
corrplot(corr.x_train, order = 'hclust')
#dev.off()
}
```

Now, we are going to conduct principal component analysis using FactorMineR and factoextra libraries:

```{r echo=TRUE, include=FALSE}
require(factoextra)
require(FactoMineR)
####PCA - using factoMiner
x_train.scale = scale(x_train.mat)

cond.PCA = PCA(x_train.scale, quanti.sup = c(24,25,26))
```

```{r}
summary(cond.PCA)
```

Below, we see the significant variables contribution to the 1st and 2nd dimensions:

```{r}
#Significant correlation of variables to the first 2 dimensions
dimdesc(cond.PCA, axes=c(1,2))
```


```{r}
#Selecting only 8 features whose quality of representaiton was the highest:
fviz_pca_var(cond.PCA, repel = T, select.var = list(cos2 = 10), col.var = "cos2")
```

```{r fig.height=5, fig.width=10, warning=FALSE}
fviz_pca_biplot(cond.PCA, repel = F,
                #Variables
                select.var = list(contrib=10),
                #alpha.var = "contrib", 
                col.var = "cos2",
                #gradient.cols = "RdYlBu",
                #Individuals
                select.ind = list(contrib = 1000),
                geom.ind = "point",
                #fill.ind = cond_train_new_vars$spacegroup, col.ind = "black",
                pointshape = 21, pointsize = 2,
                palette = "jco",
                addEllipses = TRUE,
                #col.var = "contrib", 
                legend.title = list(fill = "spacegroup", color = "Quality of Representation",
                                    alpha = "Cos2"),
                cex=.7, 
                ggtheme = theme_minimal())
```


```{r}
#Scree plot
{fviz_screeplot(cond.PCA, addlabels = TRUE, ylim = c(0, 45))
}

```


# Model Building

## MARS

###Formation Energy Model

To start off, we will attempt a Mars Model using our new variables for just formation energy.


```{r}

drop.cols <- c('bandgap_energy_ev_log_y_plus_1','ID')

cond_train3 %>%
  select(-all_of(drop.cols)) -> cond_train3_form



require(earth)


form.mars = earth(formation_energy_ev_natom_log_y_plus_1~.,degree = 2, data = cond_train3_form )

summary(form.mars)

```


```{r}
plotmo(form.mars)
```

Just starting out, we have a generalized R^2 of roughly 0.85% before we start going crazing with parameterization, which is promising as far as the maximum potential goes. It also only picked 9 of the 26 predictors. Next, we will try to play around with a few setting to improve the model.

```{r}
form.mars = earth(formation_energy_ev_natom_log_y_plus_1~.,degree = 1, data = cond_train3_form )

summary(form.mars)
```
Having interaction definetly is needed as shown above.

```{r}
form.mars = earth(formation_energy_ev_natom_log_y_plus_1~.,degree = 2, nk = 27, data = cond_train3_form )

summary(form.mars)
```
After about 27 parameters, we stop gaining anything of note in terms of GRsq.


```{r}
form.mars = earth(formation_energy_ev_natom_log_y_plus_1~.,degree = 2, nk = 50, data = cond_train3_form, nprune =  20, pmethod = "exhaustive")

summary(form.mars)


```

Utilizing exhautive pruning and cutting the model down to 20 variables, we can get a comparable R^2 with only 20 terms, which should help prevent us from overfitting.


```{r}
form.mars = earth(formation_energy_ev_natom_log_y_plus_1~.,degree = 2, nk = 50, data = cond_train3_form, nfold = 20, nprune =  20, pmethod = "exhaustive" )

summary(form.mars)
```

Internal CV using 20 folds shows that we can consistantly reach this level, at least just on this set.

We will now use the evimp command to look at variable importance for this model.


```{r}
plot(form.mars)
```


```{r}
plotmo(form.mars)
```



```{r}
evimp(form.mars)

```

Looking here, we can see that percent atoms for Aluminum and Indium are the most important, with lattive vector 3 ang, 2 ang, spacegroup, and  total atoms also pulling weight among a few others. None of our new categorical variables made the cut, but that makes sense given that Mars greatly prefers numeric data.

```{r}
evimp(form.mars, trim = FALSE)
```

Looking at all of the variables, are current model uses almost all of contributing ones in some capacity other then number_of_total_atoms used. However, because that number would be correlated with the amount of other elements used (due to the nature of those elements) it was likely disbarred for colineation reasons by the model.

```{r}
plotmo(form.mars)
```
For form energy preictions using Mars, here we have our best.

###Bangap Energy

Next, we will devise a model for bandgap energy

```{r}
drop.cols <- c('formation_energy_ev_natom_log_y_plus_1','ID')

cond_train3 %>%
  select(-all_of(drop.cols)) -> cond_train3_band




band.mars = earth(bandgap_energy_ev_log_y_plus_1~.,degree = 2, data = cond_train3_band )

summary(band.mars)



```

Starting off basic with intercation, we already have 92% GRSq. We some tweaking, we may be able to create a potentially powerful model here.

```{r}
band.mars = earth(bandgap_energy_ev_log_y_plus_1~.,degree = 1, data = cond_train3_band )

summary(band.mars)

```

From the run above, we can see that interaction does not gain us a whole lot, so we may wish to remove it to keep the model simple.

```{r}

band.mars = earth(bandgap_energy_ev_log_y_plus_1~.,degree = 1, data = cond_train3_band, nk = 16 )

summary(band.mars)


```
Unlike form energy, being able to zoom in on an exact amount is more difficult, so we will instead hit it harder with pruning combinations to try and simplfy or improve the model.

```{r}
band.mars = earth(bandgap_energy_ev_log_y_plus_1~.,degree = 2, data = cond_train3_band, nk = 30, nfold = 20, nprune =  15, pmethod = "exhaustive" )

summary(band.mars)

```

After 20 exhaustive k-fold method interntal cross-validation, we found that an R^2 of aroud 0.92 was very consistant. Assuming this translates to actual prediction output, we have potentitally a very accurate model using fairly simple methodology.



```{r}
plot(band.mars)
```


```{r}
plotmo(band.mars)
```


```{r}
evimp(band.mars)

```

For predicting bandgap energy, a smaller number of variables are important. With spacegroup, percent_atom_in, vectors 1 and 3 and angles gamma and alpha being used. Percent atom in the structure though was by far the most important, being 40% of the total models power in our case.

```{r}

evimp(band.mars, trim = FALSE)

```

Compared to formation energy, way less variables were important for the Mars Model, with variables related to the core spatial structure of the observation being far more important than the chemnical makeup. In a sense, it is a bit of the inverse of formation energy as far as predicting.


```{r}

band.evimp <- evimp(band.mars)

plot(band.evimp, cex.var = 0.6)

```

Final Metrics
```{r}


#cross validation function
MARS.cv = function(response,data, degree, nk, nprune, pmethod, p=.667,B=10) {
  n <- length(response)
  MSE <- rep(0,B)
  MAE = rep(0,B)
  MAPE = rep(0,B)
  MSLE = rep(0,B)
  for (i in 1:B) {
    ss <- floor(n*p)
    sam <- sample(1:n,ss,replace=F)
    
    fit2 <- earth(response[sam]~.,degree = degree, data = data[sam,], nk = nk, nfold = 1, nprune =  nprune, pmethod = pmethod )
    
    ynew <- predict(fit2,newdata=data[-sam,])
    MSE[i] = mean((response[-sam]-ynew)^2)
    MSLE[i] = mean((log(ynew+1) - log(response[-sam]+1))^2)
    MAE[i] = mean(abs(response[-sam]-ynew))
    MAPE[i] = mean(abs(response[-sam] - ynew)/response[-sam])
  }
  cat("RMSLEP =",sqrt(mean(MSLE)),"RMSEP =",sqrt(mean(MSE)),"  MAEP=",mean(MAE),"  MAPEP=",mean(MAPE))
  cv = return(data.frame(RMSLEP = sqrt(MSLE),RMSEP=sqrt(MSE),MAEP=MAE,MAPEP=MAPE))
}


MARS.cv(response = cond_train3_band$bandgap_energy_ev_log_y_plus_1, data = cond_train3_band[,-24], nk = 30, nprune = 15, pmethod = "exhaustive", degree = 2, B = 20, p=0.667)

#RMSLEP = 0.05490808 RMSEP = 0.09487381   MAEP= 0.05763223   MAPEP= 1.313522

MARS.cv(response = cond_train3_form$formation_energy_ev_natom_log_y_plus_1, data = cond_train3_form[,-24], nk = 50, nprune = 20, pmethod = "exhaustive", degree = 2, B = 20, p=0.667)

#RMSLEP = 0.03042171 RMSEP = 0.03754247   MAEP= 0.02394626   MAPEP= 0.2698358

```


```{r eval=FALSE, include=FALSE}

# 
# response = cond_train3_band$bandgap_energy_ev_log_y_plus_1
# data = cond_train3_band[,-24]
# nk = 30
# nprune = 15
# pmethod = "exhaustive"
# degree = 2
# B = 20
# p=0.667
# 
# n <- length(response)
# 
# 
# ss <- floor(n*p)
# sam <- sample(1:n,ss,replace=F)
# 
# length(response[sam])
# #View(data[sam,])
# 
#     
#     fit2 <- earth(response[sam]~.,degree = degree, data = data[sam,], nk = nk, nfold = 1, nprune =  nprune, pmethod = pmethod )
#     
#     ynew <- predict(fit2,newdata=data[-sam,])
#     MSE = mean((response[-sam]-ynew)^2)
#     MSLE = mean((log(ynew+1) - log(response[-sam]+1))^2)
#     MAE = mean(abs(response[-sam]-ynew))
#     MAPE = mean(abs(response[-sam] - ynew)/response[-sam])

```




While the Mars results look promising, certain variables were not fully uilized, particularly are categorical ones. Treed regression may benfit more from this, and also may pick on more direct patterns the Mars model missed, so it was tried as well.

## Treed Regression

```{r}
require(rpart)
require(maptree)
require(Cubist)


form_test_x = cond_train3_form[,-24]

form_test_y = cond_train3_form[,24]

cond_cub = cubist(x = form_test_x, y = form_test_y, committees = 1)

#summary(cond_cub)
```

The summary output was shortened to just show evaluation results.

Evaluation on training data (1567 cases):

    Average  |error|          0.0236536
    Relative |error|               0.33
    Correlation coefficient        0.90


	Attribute usage:
	  Conds  Model

	   84%    86%    percent_atom_in
	   57%    87%    lattice_vector_2_ang
	   46%    95%    percent_atom_al
	   41%   100%    lattice_vector_3_ang
	   39%    45%    spacegroup
	   26%    54%    percent_atom_ga
	   11%    36%    lattice_angle_gamma_degree
	   11%           lattice_angle_alpha_degree
	   10%    48%    lattice_vector_1_ang
	    8%           lattice_angle_beta_degree


Time: 0.1 secs


From here, we can see that with no boosting a basic treed model still gets a correlation coefficent of 0.90, which is impressive given how easy the implementation is. However, playing around with nearest neighbor adjustements, we may be able to psuh this further.


```{r message=FALSE, warning=FALSE}

#cross validation function
cubist.cv = function(x,y,p=.667,B=10,committees=1,neighbors=0) {
  n <- length(y)
  MSE <- rep(0,B)
  MAE = rep(0,B)
  MAPE = rep(0,B)
  MSLE = rep(0,B)
  for (i in 1:B) {
    ss <- floor(n*p)
    sam <- sample(1:n,ss,replace=F)
    fit2 <- cubist(x[sam,],y[sam],committees=committees,neighbors=neighbors)
    ynew <- predict(fit2,newdata=x[-sam,],neighbors=neighbors)
    MSE[i] = mean((y[-sam]-ynew)^2)
    MSLE[i] = mean((log(ynew+1) - log(y[-sam]+1))^2)
    MAE[i] = mean(abs(y[-sam]-ynew))
    MAPE[i] = mean(abs(y[-sam] - ynew)/y[-sam])
  }
  cat("RMSLEP =",sqrt(mean(MSLE)),"RMSEP =",sqrt(mean(MSE)),"  MAEP=",mean(MAE),"  MAPEP=",mean(MAPE))
  cv = return(data.frame(RMSLEP = sqrt(MSLE),RMSEP=sqrt(MSE),MAEP=MAE,MAPEP=MAPE))
}

# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 0)
# RMSLEP = 0.03224218 RMSEP = 0.03927782   MAEP= 0.02463013   MAPEP= 0.2780565
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 1)
# RMSLEP = 0.03555901 RMSEP = 0.04348276   MAEP= 0.02616518   MAPEP= 0.2537234
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 2)
# RMSLEP = 0.03314788 RMSEP = 0.04045804   MAEP= 0.02460608   MAPEP= 0.2357997
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 3)
# RMSLEP = 0.03148721 RMSEP = 0.03828172   MAEP= 0.02371599   MAPEP= 0.2223311
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 4)
# RMSLEP = 0.03325151 RMSEP = 0.04083543   MAEP= 0.02441508   MAPEP= 0.2283243
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 5)
# RMSLEP = 0.03205617 RMSEP = 0.03912071   MAEP= 0.02406652   MAPEP= 0.2333697
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 6)
# RMSLEP = 0.03177122 RMSEP = 0.03893749   MAEP= 0.02381878   MAPEP= 0.219773
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 7)
# RMSLEP = 0.03295414 RMSEP = 0.0406539   MAEP= 0.02417382   MAPEP= 0.239655
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 8)
# RMSLEP = 0.03240087 RMSEP = 0.0398106   MAEP= 0.02403365   MAPEP= 0.2549661
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 9)
# RMSLEP = 0.03144095 RMSEP = 0.0382648   MAEP= 0.02347479   MAPEP= 0.2271004





```

Trying all possible values of neighbor, we can deciseivley say that using a setting of 9 bodes the best for our model. We will now use a similar process for boosting.

```{r}

# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 1, neighbors = 9)
# RMSLEP = 0.03237573 RMSEP = 0.03984964   MAEP= 0.02395975   MAPEP= 0.2510261
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 5, neighbors = 9)
# RMSLEP = 0.02893934 RMSEP = 0.03519775   MAEP= 0.02235095   MAPEP= 0.2099355
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 10, neighbors = 9)
# RMSLEP = 0.02937464 RMSEP = 0.03593288   MAEP= 0.02234299   MAPEP= 0.2145308
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 15, neighbors = 9)
# RMSLEP = 0.02851337 RMSEP = 0.03484903   MAEP= 0.0219714   MAPEP= 0.2110919
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 20, neighbors = 9)
# RMSLEP = 0.02828576 RMSEP = 0.03436924   MAEP= 0.02208001   MAPEP= 0.2271556
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 25, neighbors = 9)
# RMSLEP = 0.02847509 RMSEP = 0.03471625   MAEP= 0.02216734   MAPEP= 0.229742
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 30, neighbors = 9)
# RMSLEP = 0.0283022 RMSEP = 0.03460824   MAEP= 0.02184816   MAPEP= 0.2198067
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 35, neighbors = 9)
# RMSLEP = 0.02717477 RMSEP = 0.0330505   MAEP= 0.02148992   MAPEP= 0.2043025
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 40, neighbors = 9)
# RMSLEP = 0.02864012 RMSEP = 0.03495877   MAEP= 0.02190446   MAPEP= 0.2211497
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 45, neighbors = 9)
# RMSLEP = 0.02848043 RMSEP = 0.03473952   MAEP= 0.021976   MAPEP= 0.2266264
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 50, neighbors = 9)
# RMSLEP = 0.02855454 RMSEP = 0.034921   MAEP= 0.02209456   MAPEP= 0.2184926
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 55, neighbors = 9)
# RMSLEP = 0.02790271 RMSEP = 0.03400922   MAEP= 0.02181917   MAPEP= 0.2154055
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 60, neighbors = 9)
# RMSLEP = 0.02833183 RMSEP = 0.03463292   MAEP= 0.02193248   MAPEP= 0.2111712
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 65, neighbors = 9)
# RMSLEP = 0.02879634 RMSEP = 0.03522894   MAEP= 0.02180916   MAPEP= 0.2033285
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 70, neighbors = 9)
# RMSLEP = 0.02880328 RMSEP = 0.03512528   MAEP= 0.02213365   MAPEP= 0.2345095
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 75, neighbors = 9)
# RMSLEP = 0.02778491 RMSEP = 0.03381687   MAEP= 0.0215106   MAPEP= 0.2213219
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 80, neighbors = 9)
# RMSLEP = 0.02846212 RMSEP = 0.03467116   MAEP= 0.02176291   MAPEP= 0.2295154
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 85, neighbors = 9)
# RMSLEP = 0.02895798 RMSEP = 0.03540791   MAEP= 0.02195497   MAPEP= 0.2213394
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 90, neighbors = 9)
# RMSLEP = 0.02873599 RMSEP = 0.03510079   MAEP= 0.02202193   MAPEP= 0.2078274
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 95, neighbors = 9)
# RMSLEP = 0.02885052 RMSEP = 0.03518716   MAEP= 0.02212498   MAPEP= 0.2482671
# > 
# > 
# > 
# > cubist.cv(form_test_x,form_test_y, p = 0.667, B = 20, committees = 100, neighbors = 9)
# RMSLEP = 0.02803035 RMSEP = 0.03412854   MAEP= 0.02171941   MAPEP= 0.2296888
# 

```

A boosting value of 35 narrowly produces the best value, leaving us with our final Treed model for Formation energy

```{r}
cubist_form_final = cubist(form_test_x,form_test_y, p = 0.667, B = 20, committees = 35, neighbors = 9)

#summary(cubist_form_final)
```

Evaluation on training data (1567 cases):

    Average  |error|          0.0232752
    Relative |error|               0.32
    Correlation coefficient        0.90


	Attribute usage:
	  Conds  Model

	   70%    91%    percent_atom_in
	   57%    89%    lattice_vector_3_ang
	   45%    84%    lattice_vector_2_ang
	   39%    87%    percent_atom_al
	   36%    39%    spacegroup
	   11%    59%    lattice_vector_1_ang
	    9%    65%    percent_atom_ga
	    5%           lattice_angle_alpha_degree
	    4%    60%    lattice_angle_gamma_degree
	    2%           lattice_angle_beta_degree
	          13%    number_of_total_atoms
	          11%    atoms_40
	           2%    atoms_30
	           
RMSLEP = 0.02717477 RMSEP = 0.0330505   MAEP= 0.02148992   MAPEP= 0.2043025

R^2 = 0.81
	           
While our improvement over the base model were small, the reduction in error still is something to be considered and does still make it worth it to consider playing around with boosting and nearest neighbors.      
	     
Now we will try bandgap as a response.

```{r}
form_test_x2 = cond_train3_band[,-24]

form_test_y2 = cond_train3_band[,24]

cond_cub2 = cubist(x = form_test_x2, y = form_test_y2, committees = 1)

#summary(cond_cub2)
```
Evaluation on training data (1567 cases):

    Average  |error|          0.0605275
    Relative |error|               0.22
    Correlation coefficient        0.94


	Attribute usage:
	  Conds  Model

	   92%    95%    lattice_vector_3_ang
	   89%    80%    percent_atom_in
	   81%    82%    percent_atom_al
	   63%    65%    spacegroup
	   37%    42%    lattice_vector_1_ang
	   13%    16%    lattice_angle_gamma_degree
	   10%    57%    percent_atom_ga
	    4%    89%    lattice_vector_2_ang
	          11%    atoms_40

While an impressive start, having low error scores and a coefficent of 0.94, it still may prove useful to optimize the paramters to maximize our yields and reduce overfitting.

```{r}

# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 0)
# RMSLEP = 0.06537628 RMSEP = 0.1152551   MAEP= 0.06144679   MAPEP= 2.249768
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 1)
# RMSLEP = 0.07195803 RMSEP = 0.1242237   MAEP= 0.06637847   MAPEP= 0.755666
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 2)
# RMSLEP = 0.06775683 RMSEP = 0.1185319   MAEP= 0.06252493   MAPEP= 2.151726
# > 
# >  
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 3)
# RMSLEP = 0.06736106 RMSEP = 0.1188582   MAEP= 0.06178664   MAPEP= 1.14858
# > 
# >  
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 4)
# RMSLEP = 0.06934879 RMSEP = 0.1224337   MAEP= 0.06336363   MAPEP= 2.084187
# > 
# >  
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 5)
# RMSLEP = 0.06341158 RMSEP = 0.1137942   MAEP= 0.05969285   MAPEP= 2.539457
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 6)
# RMSLEP = 0.06681612 RMSEP = 0.1179883   MAEP= 0.06238921   MAPEP= 1.382312
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 7)
# RMSLEP = 0.06543542 RMSEP = 0.1143767   MAEP= 0.06111267   MAPEP= 1.301673
# > 
# > 
# >  
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 8)
# RMSLEP = 0.06572626 RMSEP = 0.1153895   MAEP= 0.06001655   MAPEP= 1.629123
# 
# 
# 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 9)
# RMSLEP = 0.06377824 RMSEP = 0.1139196   MAEP= 0.06060954   MAPEP= 2.925799
```

The optimal value for neighbors is 5. Now onto boosting.


```{r}
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 1, neighbors = 5)
# RMSLEP = 0.06568027 RMSEP = 0.1131044   MAEP= 0.06050445   MAPEP= 2.551471
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 5, neighbors = 5)
# RMSLEP = 0.05646231 RMSEP = 0.09855742   MAEP= 0.05619269   MAPEP= 2.47747
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 10, neighbors = 5)
# RMSLEP = 0.0557989 RMSEP = 0.09769368   MAEP= 0.05582741   MAPEP= 1.76166
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 15, neighbors = 5)
# RMSLEP = 0.05732295 RMSEP = 0.09926732   MAEP= 0.05657491   MAPEP= 1.373725
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 20, neighbors = 5)
# RMSLEP = 0.0581008 RMSEP = 0.1000819   MAEP= 0.05658944   MAPEP= 2.448342
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 25, neighbors = 5)
# RMSLEP = 0.05757621 RMSEP = 0.1004281   MAEP= 0.05637488   MAPEP= 2.383588
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 30, neighbors = 5)
# RMSLEP = 0.05798935 RMSEP = 0.1000757   MAEP= 0.05589889   MAPEP= 2.954169
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 35, neighbors = 5)
# RMSLEP = 0.05785085 RMSEP = 0.1006174   MAEP= 0.05638628   MAPEP= 1.453625
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 40, neighbors = 5)
# RMSLEP = 0.05616211 RMSEP = 0.09804356   MAEP= 0.05557444   MAPEP= 1.534716
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 45, neighbors = 5)
# RMSLEP = 0.05548687 RMSEP = 0.09659392   MAEP= 0.05467256   MAPEP= 2.219598
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 50, neighbors = 5)
# RMSLEP = 0.05736875 RMSEP = 0.0986013   MAEP= 0.05579035   MAPEP= 2.864756
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 55, neighbors = 5)
# RMSLEP = 0.05785987 RMSEP = 0.1010301   MAEP= 0.05638847   MAPEP= 2.534446
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 60, neighbors = 5)
# RMSLEP = 0.05546727 RMSEP = 0.09642195   MAEP= 0.05523967   MAPEP= 2.568528
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 65, neighbors = 5)
# RMSLEP = 0.05788636 RMSEP = 0.1001062   MAEP= 0.05632725   MAPEP= 1.455314
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 70, neighbors = 5)
# RMSLEP = 0.05740603 RMSEP = 0.1005015   MAEP= 0.05679506   MAPEP= 2.144069
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 75, neighbors = 5)
# RMSLEP = 0.0563129 RMSEP = 0.09802249   MAEP= 0.05560001   MAPEP= 1.443422
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 80, neighbors = 5)
# RMSLEP = 0.05729028 RMSEP = 0.09899598   MAEP= 0.05590552   MAPEP= 2.46016
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 85, neighbors = 5)
# RMSLEP = 0.05538537 RMSEP = 0.09533345   MAEP= 0.05419751   MAPEP= 1.591038
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 90, neighbors = 5)
# RMSLEP = 0.05892291 RMSEP = 0.1018736   MAEP= 0.05710835   MAPEP= 2.26647
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 95, neighbors = 5)
# RMSLEP = 0.05499022 RMSEP = 0.09668556   MAEP= 0.05557619   MAPEP= 2.159504
# > 
# > 
# > 
# > cubist.cv(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 100, neighbors = 5)
# RMSLEP = 0.05631326 RMSEP = 0.09833671   MAEP= 0.0559818   MAPEP= 2.106057

```

The best model form bandgap energy uses 85 boosts according to our work above, meaning our best model via treed is shown below.

```{r}
cubist_band_final = cubist(form_test_x2,form_test_y2, p = 0.667, B = 20, committees = 85, neighbors = 5)

#summary(cubist_band_final)
```

Evaluation on training data (1567 cases):

    Average  |error|          0.0594649
    Relative |error|               0.22
    Correlation coefficient        0.94


	Attribute usage:
	  Conds  Model

	   83%    91%    lattice_vector_3_ang
	   74%    93%    percent_atom_in
	   59%    82%    percent_atom_al
	   28%    47%    spacegroup
	    8%    67%    lattice_angle_gamma_degree
	    7%           lattice_angle_alpha_degree
	    6%    84%    lattice_vector_2_ang
	    5%    66%    lattice_vector_1_ang
	    4%    75%    percent_atom_ga
	    2%           lattice_angle_beta_degree
	          31%    number_of_total_atoms
	          29%    Spacegroup_167
	          23%    atoms_40
	          11%    atoms_30
	           3%    atoms_60

RMSLEP = 0.05538537 RMSEP = 0.09533345   MAEP= 0.05419751   MAPEP= 1.591038

R^2 = 0.8836

Much like our model for formation energy, our direct metrics based such as error have only improved slightly, and our correlation coefficent did not change much. But these improvements still are measureable and therefore should be taken into account.

## XGboost
Another tree-based method tried was XGboost, as we thought that it was possible that the regression based approch of the treed model could potentially be holding it back given the difficult nature of our predictors.

Like before, we will start with Form garp as our response.

```{r}

require(xgboost)


form_test_y_list = as.list(form_test_y)

form_test_x_mat = as.matrix(form_test_x)

basic_boost_form = xgboost(data = form_test_x_mat, label = form_test_y_list, nrounds = 30, nthread = 4, verbose = T, objective = "reg:linear", eta = 0.9, max.depth = 6 )



```

While these intial results look good with just basic parameters, we still need to try and optimize the model by changing them. We will us K-folds cross-validation to get those results.

```{r}
# params = list(booster="gbtree",objective="reg:squaredlogerror",eta=0.9,gamma=0,
# max_depth=5,min_child_weight=80,subsample=.66,colsample_bytree=1,
# lambda=0.80,alpha=0.55)
# 
# 
# from_xg_cv_results = xgb.cv(params = params, data = form_test_x_mat, nrounds = 500, print_every_n = 10, nthreads = 4, nfold = 20, stratified = F, early_stopping_rounds = 30, maximize = 0, label = form_test_y_list) 
```

[1]	train-rmsle:0.145127+0.002620	test-rmsle:0.144935+0.009187 
Multiple eval metrics are present. Will use test_rmsle for early stopping.
Will train until test_rmsle hasn't improved in 30 rounds.

[11]	train-rmsle:0.033935+0.000620	test-rmsle:0.035315+0.004063 
[21]	train-rmsle:0.030586+0.000534	test-rmsle:0.032836+0.004206 
[31]	train-rmsle:0.029062+0.000438	test-rmsle:0.031696+0.004325 
[41]	train-rmsle:0.028140+0.000436	test-rmsle:0.030978+0.004436 
[51]	train-rmsle:0.027481+0.000403	test-rmsle:0.030678+0.004610 
[61]	train-rmsle:0.027002+0.000363	test-rmsle:0.030468+0.004622 
[71]	train-rmsle:0.026560+0.000380	test-rmsle:0.030222+0.004620 
[81]	train-rmsle:0.026210+0.000363	test-rmsle:0.030137+0.004666 
[91]	train-rmsle:0.025918+0.000362	test-rmsle:0.029991+0.004657 
[101]	train-rmsle:0.025651+0.000355	test-rmsle:0.029905+0.004694 
[111]	train-rmsle:0.025428+0.000356	test-rmsle:0.029781+0.004759 
[121]	train-rmsle:0.025224+0.000343	test-rmsle:0.029680+0.004743 
[131]	train-rmsle:0.025017+0.000350	test-rmsle:0.029600+0.004719 
[141]	train-rmsle:0.024851+0.000344	test-rmsle:0.029529+0.004716 
[151]	train-rmsle:0.024693+0.000349	test-rmsle:0.029572+0.004690 
[161]	train-rmsle:0.024544+0.000351	test-rmsle:0.029538+0.004799 
[171]	train-rmsle:0.024389+0.000348	test-rmsle:0.029471+0.004873 
[181]	train-rmsle:0.024262+0.000356	test-rmsle:0.029406+0.004858 
[191]	train-rmsle:0.024139+0.000345	test-rmsle:0.029332+0.004830 
[201]	train-rmsle:0.024022+0.000341	test-rmsle:0.029341+0.004875 
[211]	train-rmsle:0.023910+0.000328	test-rmsle:0.029309+0.004926 
[221]	train-rmsle:0.023802+0.000323	test-rmsle:0.029305+0.004913 
[231]	train-rmsle:0.023687+0.000335	test-rmsle:0.029237+0.004862 
[241]	train-rmsle:0.023578+0.000320	test-rmsle:0.029244+0.004969 
[251]	train-rmsle:0.023477+0.000306	test-rmsle:0.029220+0.004972 
[261]	train-rmsle:0.023398+0.000303	test-rmsle:0.029251+0.004904 
[271]	train-rmsle:0.023301+0.000306	test-rmsle:0.029276+0.004927 
Stopping. Best iteration:
[246]	train-rmsle:0.023528+0.000314	test-rmsle:0.029176+0.004953


After attempting to modify parameters to beat the Treed regression model, the closest that could be gotten conistantly was the one above with 256 rounds for and RMSLE of 0.029176.

Other Metrics:

RMSE: 0.036691

MAE: 0.024237

Final Model:
```{r include=FALSE}
XG_form = xgboost(data = form_test_x_mat, label = form_test_y_list, nrounds = 256, nthread = 4, verbose = T, objective = "reg:squaredlogerror", eta = 0.9, max.depth = 5, lambda = 0.8, alpha = 0.55, stratified = F, gamma = 0, colsample_bytree=1, maximize = 0, min_child_weight=80)
```


Looking at variable importance for top ten predictors:

```{r}
mat = xgb.importance(feature_names=colnames(form_test_x_mat),model=XG_form)
xgb.plot.importance(importance_matrix=mat[1:10])

```

For all predictors:

```{r}
mat = xgb.importance(feature_names=colnames(form_test_x_mat),model=XG_form)
xgb.plot.importance(importance_matrix=mat[1:10])
```



Next, we will work similarly on bandgap.

```{r}



form_test_y2_list = as.list(form_test_y2)

form_test_x2_mat = as.matrix(form_test_x2)

basic_boost_form = xgboost(data = form_test_x_mat, label = form_test_y_list, nrounds = 30, nthread = 4, verbose = T, objective = "reg:linear", eta = 0.9, max.depth = 6 )


```

While even better then our initial run through from before, we still will go through the same process.

```{r}

# 
# params = list(booster="gbtree",objective="reg:squaredlogerror",eta=0.4,gamma=0,
# max_depth=1,min_child_weight=15,subsample=.66,colsample_bytree=0.75,
# lambda=0.5,alpha=0)
# 
# 
# from_xg_cv_results2 = xgb.cv(params = params, data = form_test_x2_mat, nrounds = 500, print_every_n = 10, nthreads = 7, nfold = 20, stratified = F, early_stopping_rounds = 30, maximize = 0, label = form_test_y2_list)

```


[1]	train-rmsle:0.267417+0.000682	test-rmsle:0.267147+0.012896 
Multiple eval metrics are present. Will use test_rmsle for early stopping.
Will train until test_rmsle hasn't improved in 30 rounds.

[11]	train-rmsle:0.075467+0.000847	test-rmsle:0.075456+0.012180 
[21]	train-rmsle:0.061307+0.000904	test-rmsle:0.061674+0.011922 
[31]	train-rmsle:0.057481+0.001031	test-rmsle:0.058701+0.011041 
[41]	train-rmsle:0.055696+0.000976	test-rmsle:0.057283+0.011041 
[51]	train-rmsle:0.054432+0.000859	test-rmsle:0.056481+0.011143 
[61]	train-rmsle:0.053474+0.000824	test-rmsle:0.055702+0.011280 
[71]	train-rmsle:0.052698+0.000756	test-rmsle:0.055063+0.011169 
[81]	train-rmsle:0.052084+0.000742	test-rmsle:0.054496+0.011229 
[91]	train-rmsle:0.051502+0.000704	test-rmsle:0.054383+0.011125 
[101]	train-rmsle:0.051016+0.000681	test-rmsle:0.054262+0.011255 
[111]	train-rmsle:0.050615+0.000703	test-rmsle:0.054092+0.011103 
[121]	train-rmsle:0.050237+0.000691	test-rmsle:0.053993+0.011396 
[131]	train-rmsle:0.049917+0.000685	test-rmsle:0.053875+0.011476 
[141]	train-rmsle:0.049621+0.000691	test-rmsle:0.053708+0.011328 
[151]	train-rmsle:0.049317+0.000675	test-rmsle:0.053769+0.011501 
[161]	train-rmsle:0.049029+0.000642	test-rmsle:0.053776+0.011665 
[171]	train-rmsle:0.048782+0.000629	test-rmsle:0.053591+0.011670 
[181]	train-rmsle:0.048542+0.000614	test-rmsle:0.053580+0.011512 
[191]	train-rmsle:0.048309+0.000612	test-rmsle:0.053621+0.011569 
[201]	train-rmsle:0.048122+0.000604	test-rmsle:0.053479+0.011579 
[211]	train-rmsle:0.047936+0.000596	test-rmsle:0.053238+0.011461 
[221]	train-rmsle:0.047741+0.000586	test-rmsle:0.053430+0.011315 
[231]	train-rmsle:0.047560+0.000564	test-rmsle:0.053464+0.011292 
Stopping. Best iteration:
[210]	train-rmsle:0.047958+0.000605	test-rmsle:0.053097+0.011444


After many iterative cycles of modifciation, the above model won out in cross-validation having gone 210 rounds for and RMSLE of 0.053097.

Other Metrics:

RMSE: 0.04795765

MAE: 0.062172

Final Model:


```{r include=FALSE}
XG_band = xgboost(data = form_test_x2_mat, label = form_test_y2_list, nrounds = 210, nthread = 7, verbose = T, objective = "reg:squaredlogerror", eta = 0.4, max.depth = 1, lambda = 0.5, alpha = 0, stratified = F, gamma = 0, colsample_bytree=1, maximize = 0, min_child_weight=15)
```


Variable importance for final model for bandgap energy using top 10 predictors.


```{r}
mat = xgb.importance(feature_names=colnames(form_test_x2_mat),model=XG_band)
xgb.plot.importance(importance_matrix=mat[1:10])

```

Looking at all predictors

```{r}
mat = xgb.importance(feature_names=colnames(form_test_x2_mat),model=XG_band)
xgb.plot.importance(importance_matrix=mat[1:26])
```


##Shrinkage methods: ridge, lasso, Elastic Net

```{r include=FALSE}
######### Libraries ######
{
  library(tidyr)
  library(dplyr)
  library(ggplot2)
  library(nnet)
  #library(Ecfun)
  library(car)
  library(ISLR)
  #library(MASS)
  library(glmnet)
  library(pls)
  library(corrplot)
  require(stringr)
  library(gridExtra)
  library(ipred)
  library(randomForest)
}
```

```{r echo=TRUE}
###### Monte Carlo Cross-Validation of Elastic Net, Ridge and Lasso Regression ####
{glmnet.ssmc = function(X,y,p=.667,M=100,alpha=1,lambda=1) {
  RMSEP = rep(0,M)
  RMSLEP = rep(0,M)
  MAEP = rep(0,M)
  MAPEP = rep(0,M)
  n = nrow(X)
  for (i in 1:M) {
    ss = floor(n*p)
    sam = sample(1:n,ss,replace=F)
    fit = glmnet(X[sam,],y[sam],lambda=lambda,alpha=alpha)
    ypred = predict(fit,newx=X[-sam,])
    RMSEP[i] = sqrt(mean((y[-sam]-ypred)^2))
    RMSLEP[i] = sqrt(mean((log(ypred +1) - log(y[-sam] +1))^2)) 
    MAEP[i] = mean(abs(y[-sam]-ypred))
    yp = ypred[y[-sam]!=0]
    ya = y[-sam][y[-sam]!=0]
    MAPEP[i]=mean(abs(yp-ya)/ya)
  }
  cat("RMSEP=",mean(RMSEP),
      "RMSLEP=",mean(RMSLEP),
      "MAEP=",mean(MAEP),
      "MAPEP=",mean(MAPEP))
  cv = return(data.frame(RMSEP=RMSEP, RMSLEP = RMSLEP, MAEP=MAEP,MAPEP=MAPEP)) 
}
}
```

```{r}
###### Load and prepare data ######
#Load data:
{#setwd("~/OneDrive - MNSCU/myGithub/Supervised_Learning/Multiple_Linear_Regression/MachineLearning-SupervisedLearning/Conductor Formulations")
  #Train set
  cond_train = read.csv("Conductors (train).csv")
  #Test set
  cond_test = read.csv("Conductors (test).csv")
  cond_test_final = read.csv("cond_test_new_variables.csv")
  ### New training sets
  cond_train_new_vars = read.csv("cond_train_new_variables.csv")
  #No_box_cox
  #angles measures are still in degrees; no transformaitons on predictors
  cond_train_new_vars_no_box_cox = read.csv("cond_train_new_variables_no_box_cox.csv")
  
  ####### New Variables #######
  
  #Resposne vectors 
  y_train_form_eng_trans = cond_train_new_vars$formation_energy_ev_natom_log_y_plus_1
  y_train_bandgap_eng_trans = cond_train_new_vars$bandgap_energy_ev_log_y_plus_1
  
  #Resposne vectors for no box_cox
  #y_train_form_eng_trans = cond_train_new_vars_no_box_cox$formation_energy_ev_natom_log_y_plus_1
  #y_train_bandgap_eng_trans = cond_train_new_vars_no_box_cox$bandgap_energy_ev_log_y_plus_1
  
  #Df of all x's
  x_train = cond_train_new_vars[,-c(1,25,26)]
  #x_train =cond_train_new_vars_no_box_cox[,-c(1,2,26,27)]
}
#Rearrange the data for two models: Model 1: formation energy as response; Model 2: bandgap energy as response
{#detach("package:Ecfun", unload = TRUE)
  cond_train_new_vars %>%
    select(25,2:24,27:29) -> train_model1
  
  cond_train_new_vars%>%
    select(26, 2:24,27:29) -> train_model2
} 
#Rearrange the data for two models for no box_cox
{
  #detach("package:Ecfun", unload = TRUE)
  cond_train_new_vars_no_box_cox %>%
    select(26,3:25,28:30) -> train_model1_no_box_cox
  
  cond_train_new_vars_no_box_cox%>%
    select(27, 3:25,28:30) -> train_model2_no_box_cox
} 
```

### Model 1: response is formation energy
```{r}
######## Model 1:response is formation energy ########
####### Set up #########
{
  X = scale(model.matrix(formation_energy_ev_natom_log_y_plus_1~., data = train_model1)[,-1])
  y = train_model1$formation_energy_ev_natom_log_y_plus_1
}
```

Below we go through the process of finding the optimal alpha value for Elastic Net:

```{r include=FALSE}
###### For Elastic Net: finding lambda and fitting the model ########
#Find best lambda
{ 
  # par(mfrow=c(4,4))
  # #Change alpha
  # alpha = 0.8
  # cv.en = cv.glmnet(X,y,alpha=alpha)
  # bestlam.en = cv.en$lambda.min
  #plot(cv.en)
  #title(main = paste("Best log(lambda) for Elastic Net", (round(log(bestlam.en),digits = 2))), sub = paste("Best lambda:", round(bestlam.en,5)))
}
#Fit with optimal lambda
{
  # en.mod = glmnet(X,y,alpha=alpha, lambda =bestlam.en)
  
  #y and yhat correlation
  # en.cor = cor(y, predict(en.mod, newx = X))
  
  #y and yhat correlation^2 = R^2
  # en.rsqaured = cor(y, predict(en.mod, newx = X))^2
  # plot(y,predict(en.mod,newx=X),xlab="Actual Age (y-values)",ylab="Predicted Age (yhat-values)", main = paste("Elastic Net Model:", "   ", "Correlation:", round(en.cor,4), "   ", "R^2:", round(en.rsqaured,4)),
  #      sub = paste("alpha:", " ", alpha))
}

```

Now, we are going to fit Elastic Net models with alphas of 0.95, 0.5, 0.05.

```{r}
###### Elastic net #########
{alpha1 = 0.95
cv.en = cv.glmnet(X,y,alpha=alpha1)
bestlam.en = cv.en$lambda.min

alpha2 = 0.5
cv.en = cv.glmnet(X,y,alpha=alpha2)
bestlam.en = cv.en$lambda.min

alpha3 = 0.05
cv.en = cv.glmnet(X,y,alpha=alpha3)
bestlam.en = cv.en$lambda.min
#plot(cv.en)
#title(main = paste("Best log(lambda) for Elastic Net", (round(log(bestlam.en),digits = 2))), sub = paste("Best lambda:", round(bestlam.en,5)))
en.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=alpha1,lambda=bestlam.en)
en.results2 = glmnet.ssmc(X,y,p=.75, M=1000,alpha=alpha2,lambda=bestlam.en)
en.results3 = glmnet.ssmc(X,y,p=.75, M=1000,alpha=alpha3,lambda=bestlam.en)
}
```

Now, we are fitting ridge regression model:

```{r}
###### Ridge regression##########
{cv.ridge= cv.glmnet(X, y, alpha = 0)
bestlam.ridge = cv.ridge$lambda.min
#plot(cv.ridge)
#title(main = paste("Best log(lambda) for Ridge", (round(log(bestlam.ridge),digits = 2))), sub = paste("Best lambda:", round(bestlam.ridge)))
ridge.results = glmnet.ssmc(X,y, p=.75, M=1000,alpha=0,lambda=bestlam.ridge)
}
```

Lastly, we are fitting lasso regression model:
```{r}
###### Lasso regression##############
{cv.lasso = cv.glmnet(X, y, alpha = 1)
bestlam.lasso = cv.lasso$lambda.min
#plot(cv.lasso)
#title(main = paste("Best log(lambda) for Lasso", (round(log(bestlam.lasso),digits = 4))), sub = paste("Best lambda:", round(bestlam.lasso, 4)))
lasso.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=1,lambda=bestlam.lasso)
}
```

Now, let's compare the three different shrinkage methods we used:
```{r}
###### Compare the three methods ###########
{names = c("Ridge", "Lasso", "Elastic Net, alpha = 0.95", "Elastic Net, alpha = 0.5", "Elastic Net, alpha = 0.0.005")
metrics =cbind(((names)), rbind(
  do.call(cbind, lapply(ridge.results,mean)), 
  do.call(cbind, lapply(lasso.results, mean)),
  do.call(cbind, lapply(en.results, mean)),
  do.call(cbind, lapply(en.results2, mean)),
  do.call(cbind, lapply(en.results3, mean))))

df.metrics = as.data.frame(metrics)
#write.csv(df.metrics, file = "Model_1_shrinkage_regression_results.csv", row.names = FALSE)
df.metrics[order(df.metrics$RMSLEP),]
}

```

The winner amongst the three shrinkage methods for Model 1 (reposne: formation energy) is Lasso (RMSLEP=0.0412)

### Model 2: response is bandgap energy
```{r}
###### Model 2:response is bandgap energy ########
###### Set up #########
{
X = scale(model.matrix(bandgap_energy_ev_log_y_plus_1~., data = train_model2)[,-1])
y = train_model2$bandgap_energy_ev_log_y_plus_1
}
```

Below we go through the process of finding the optimal alpha value for Elastic Net:
```{r}
###### For Elastic Net: finding lambda and fitting the model ########
#Find best lambda
{ 
  # par(mfrow=c(4,4))
  #Change alpha
  # alpha = 0.8
  # cv.en = cv.glmnet(X,y,alpha=alpha)
  # bestlam.en = cv.en$lambda.min
  #plot(cv.en)
  #title(main = paste("Best log(lambda) for Elastic Net", (round(log(bestlam.en),digits = 2))), sub = paste("Best lambda:", round(bestlam.en,5)))
  #en.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=0.1,lambda=bestlam.en)
}
#Fit with optimal lambda
{
  # en.mod = glmnet(X,y,alpha=alpha, lambda =bestlam.en)
  # 
  # #y and yhat correlation
  # en.cor = cor(y, predict(en.mod, newx = X))
  # 
  # #y and yhat correlation^2 = R^2
  # en.rsqaured = cor(y, predict(en.mod, newx = X))^2
  # plot(y,predict(en.mod,newx=X),xlab="Actual Age (y-values)",ylab="Predicted Age (yhat-values)", main = paste("Elastic Net Model:", "   ", "Correlation:", round(en.cor,4), "   ", "R^2:", round(en.rsqaured,4)),
  #      sub = paste("alpha:", " ", alpha))
}
```

Now, we are going to fit Elastic Net models with alphas of 0.5, 0.4, 0.8.

```{r}
###### Elastic net #########
{alpha1 = 0.8
cv.en = cv.glmnet(X,y,alpha=alpha1)
bestlam.en = cv.en$lambda.min

alpha2 = 0.5
cv.en = cv.glmnet(X,y,alpha=alpha2)
bestlam.en = cv.en$lambda.min

alpha3 = 0.4
cv.en = cv.glmnet(X,y,alpha=alpha3)
bestlam.en = cv.en$lambda.min
#plot(cv.en)
#title(main = paste("Best log(lambda) for Elastic Net", (round(log(bestlam.en),digits = 2))), sub = paste("Best lambda:", round(bestlam.en,5)))
en.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=alpha1,lambda=bestlam.en)
en.results2 = glmnet.ssmc(X,y,p=.75, M=1000,alpha=alpha2,lambda=bestlam.en)
en.results3 = glmnet.ssmc(X,y,p=.75, M=1000,alpha=alpha3,lambda=bestlam.en)
}
```

Now, we are fitting ridge regression model:

```{r}
###### Ridge regression##########
{cv.ridge= cv.glmnet(X, y, alpha = 0)
bestlam.ridge = cv.ridge$lambda.min
#plot(cv.ridge)
#title(main = paste("Best log(lambda) for Ridge", (round(log(bestlam.ridge),digits = 2))), sub = paste("Best lambda:", round(bestlam.ridge)))
ridge.results = glmnet.ssmc(X,y, p=.75, M=1000,alpha=0,lambda=bestlam.ridge)
}
```

Lastly, we are fitting lasso regression model:

```{r}
###### Lasso regression##############
{cv.lasso = cv.glmnet(X, y, alpha = 1)
bestlam.lasso = cv.lasso$lambda.min
#plot(cv.lasso)
#title(main = paste("Best log(lambda) for Lasso", (round(log(bestlam.lasso),digits = 4))), sub = paste("Best lambda:", round(bestlam.lasso, 4)))
lasso.results = glmnet.ssmc(X,y,p=.75, M=1000,alpha=1,lambda=bestlam.lasso)
}
```

Now, let's compare the three different shrinkage methods we used:

```{r}
###### Compare the three methods ###########
{names = c("Ridge", "Lasso", "Elastic Net, alpha = 0.8", "Elastic Net, alpha = 0.5", "Elastic Net, alpha = 0.4")
metrics =cbind(((names)), rbind(
  do.call(cbind, lapply(ridge.results,mean)), 
  do.call(cbind, lapply(lasso.results, mean)),
  do.call(cbind, lapply(en.results, mean)),
  do.call(cbind, lapply(en.results2, mean)),
  do.call(cbind, lapply(en.results3, mean))))

df.metrics = as.data.frame(metrics)
#write.csv(df.metrics, file = "Model_2_shrinkage_regression_results.csv", row.names = FALSE)
df.metrics[order(df.metrics$RMSLEP),]
}

```

The winner amongst the three shrinkage methods for Model 2 (reposne: bandgap energy) is, as well, Lasso (RMSLEP=0.0571)

The shrinkage methods did not perform better than MARS, Treed Regression or XGboosting for Model 1 (resposne=formation energy); for Model 2 (response=bandgap energy) Lasso and Elastic Net (alpha=0.5) did not do that bad. We will revist all the models' RMSLEP metrics at the end.

We still must consider Random Forest.

## Random Forest

```{r include=FALSE}
###### Libraries #####
{
  library(tidyr)
  library(dplyr)
  library(ggplot2)
  library(nnet)
  #library(Ecfun)
  library(car)
  library(ISLR)
  #library(MASS)
  library(glmnet)
  library(pls)
  library(corrplot)
  require(stringr)
  library(gridExtra)
  library(ipred)
  library(randomForest)
}

```

```{r}
###### Functions #####
#detach("package:Ecfun", unload = TRUE)
#rf.sscv Monte-Carlo CV function
{rf.sscv = function(fit,data,p=.667,B=100,mtry=fit$mtry,ntree=fit$ntree) {
  RMSEP = rep(0,B)
  RMSLEP = rep(0,B)
  MAEP = rep(0,B)
  MAPEP = rep(0,B)
  y = fit$y
  n = nrow(data)
  ss <- floor(n*p)
  for (i in 1:B) {
    sam = sample(1:n,ss,replace=F)
    fit2 = randomForest(formula(fit),data=data[sam,],mtry=mtry,ntree=ntree)
    ynew = predict(fit2,newdata=data[-sam,])
    RMSEP[i] = sqrt(mean((y[-sam]-ynew)^2))
    RMSLEP[i] = sqrt(mean((log(ynew +1) - log(y[-sam] +1))^2))
    MAEP[i] = mean(abs(y[-sam]-ynew))
    MAPEP[i] = mean((abs(y[-sam]-ynew)/y[-sam]))
    
  }
  RMSEP = mean(RMSEP)
  RMSLEP = mean(RMSLEP)
  MAEP = mean(MAEP)
  MAPEP = mean(MAPEP)
  cat("RMSEP\n")
  cat("===============\n")
  cat(RMSEP,"\n\n")
  cat("RMSLEP\n")
  cat("===============\n")
  cat(RMSLEP,"\n\n")
  cat("MAEP\n")
  cat("===============\n")
  cat(MAEP,"\n\n")
  cat("MAPEP\n")
  cat("===============\n")
  cat(MAPEP,"\n\n")
  temp = data.frame(RMSEP=RMSEP, RMSLEP=RMSLEP, MAEP=MAEP,MAPEP=MAPEP)
  return(temp)
}

}
#Predict Accuracy function
{PredAcc = function(y, ypred){
  RMSEP = sqrt(mean((y-ypred)^2))
  RMSLEP = sqrt(mean((log(ypred +1) - log(y+1))^2))
  MAE = mean(abs(y-ypred))
  MAPE = mean(abs(y-ypred)/y)*100
  cat("RMSEP\n")
  cat("================\n")
  cat(RMSEP, "\n\n")
  cat("RMSLEP\n")
  cat("================\n")
  cat(RMSLEP, "\n\n")
  cat("MAE\n")
  cat("================\n")
  cat(MAE, "\n\n")
  cat("MAPE\n")
  cat("================\n")
  cat(MAPE, "\n\n")
  return(data.frame(RMSEP = RMSEP, RMSLEP = RMSLEP, MAE = MAE, MAPE = MAPE))
}
}
#Plotting variable importance
{rfimp = function(rffit, horiz=T) {barplot(sort(rffit$importance[,1]),horiz=horiz,
                                 xlab="Mean Decreasing in Accuracy",main="Variable Importance")
}
}

```

The data was loaded and prepared in the Shirnkage Methods section. We are going to continue working with train_model1 and train_model2 data frames.

### Model 1

```{r}
###### Model 1 #####
{y = train_model1$formation_energy_ev_natom_log_y_plus_1
X = train_model1
cond.rf = randomForest(y~.,data=X,importance=F)
}
```

We are splitting our training set into training and validation sets:
```{r}

####### Create validation set #######
#trainmodel1 is X
#response is y
{n = nrow(X)
set.seed(2019)
sam = sample(1:n,size=floor(n*.6667),replace=F)
X.train = X[sam,] #form training dataset
X.valid = X[-sam,] #form validation dataset
y.train = X.train$formation_energy_ev_natom_log_y_plus_1
y.valid = X.valid$formation_energy_ev_natom_log_y_plus_1

#cond.rf = randomForest(y.train~.,data=X.train)
#plot(cond.rf)
#abline(h=0.000128,col="red",lwd=2)
} 
```

Now we are going to go through the process of finding optimal number of trees (ntree) and number of variables to randomly pick at each stage (mtry)

```{r}
#1st: choose optimal mtry's

# results = rf.sscv(cond.rf,X,mtry=3)
# results = rf.sscv(cond.rf,X,mtry=4)
# results = rf.sscv(cond.rf,X,mtry=5)
# results = rf.sscv(cond.rf,X,mtry=6)

#Compare mtry's with errorest function
{#m=4
# m=4
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
# #m=5
# m=5
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
# #m=6
# m=6
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
# 
# #m=7
# m=7
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
}
#2nd: Increasing the number of trees beyonf the default ntree=500
#BEST = based on best mtry from above
#BEST = 5

# results = rf.sscv(cond.rf,X,mtry=BEST,ntree=500,B=250)
# results = rf.sscv(cond.rf,X,mtry=BEST,ntree=750,B=250)
# results = rf.sscv(cond.rf,X,mtry=BEST,ntree=1000,B=250)


```

```{r}
###### Compare best mtry and ntree combos #######                        
#ntree of 50, 80, 100, 350

# cond.rf = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,ntree=50, mtry=9)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,ntree=80)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,ntree=100)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,ntree=350)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,ntree=150, mtry =12)
# results = rf.sscv(cond.rf,X.train)

#From best to least: ntree = 350, then 80, then 50, then 100

#mtry

# cond.rf = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,mtry = 6,ntree=1000)
# results = rf.sscv(cond.rf,X.train)
# cond.final = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,mtry=5,ntree=750)
# results = rf.sscv(cond.rf,X.train)

#mtry=5, ntree=350
#mtry=5, ntree=750
#mtry=6, ntree=1000
#mtry=6, ntree=350
```

Our thorough process of exploring different number of bootstrap trees and number of variables to randomly pick at each stage gave us the following optimal hyperparameters: Best combo is: <b> mtry=5 and ntree=350 </b>

This gives us our final model:
```{r}
####### Final randomForest model #######
{cond.final = randomForest(formation_energy_ev_natom_log_y_plus_1~.,data=X.train,mtry=5,ntree=350, importance = T)
ypred = predict(cond.final,newdata=X.valid)
prediciton.rf = PredAcc(y.valid,ypred)
}
```

Let's see how our final random forest model predicts. We are plotting the actual formation energy values vs. predicted for both the training set data (figure on the left) and validaiton set data (figure on the right)

```{r fig.height=5, fig.width=10}
par(mfrow=c(1,2))
plot(y.train,predict(cond.final),xlab="Actual Formation Energy",ylab="Fitted Formation Energy",
     main= paste("Training Data", " R^2: ", 
                 round((cor(y.train, 
                            predict(cond.final))^2),4)))
abline(0,1,col="red",lwd=2)

plot(y.valid, ypred, xlab="Actual Formation Energy",ylab="Predicted Formation Energy",
     main=paste("Test Data", "R^2: ",
                round((cor(y.valid, 
                           ypred)^2),4)))
abline(0,1,col="red",lwd=2)
#par(mfrow=c(1,1))
```

Our final random forest model predicts with accuracy of RMSLEP=0.0268 and gives us 86.35% rsquared value. It is by far the best model for predicting formation energy.

#### Predicting on Test Set

<b> Prediction on Test Set </b>
```{r}
randomforest.final = cond.final
```

```{r}
cond_test_final %>%
  select (-c(bandgap_energy_ev_log_y_plus_1,formation_energy_ev_natom_log_y_plus_1,ID)) -> Modified_Xs
```


```{r}
Formation_Predictions = predict(randomforest.final, as.matrix(Modified_Xs))
write.csv(Formation_Predictions, file = "formation_predictions_RandomForest.csv")
```


### Model 2 


```{r}
############ Model 2 #################
{y = train_model2_no_box_cox$bandgap_energy_ev_log_y_plus_1
X = train_model2_no_box_cox
cond.rf = randomForest(y~.,data=X,importance=T)
}
```

We are splitting our training set into training and validation sets:
```{r}
####### Create validation set #######
#trainmodel2 is X
#response is y
{n = nrow(X)
set.seed(2019)
sam = sample(1:n,size=floor(n*.6667),replace=F)
X.train = X[sam,] #form training dataset
X.valid = X[-sam,] #form validation dataset
y.train = X.train$bandgap_energy_ev_log_y_plus_1
y.valid = X.valid$bandgap_energy_ev_log_y_plus_1

# par(mfrow=c(1,1))
# cond.rf = randomForest(y.train~.,data=X.train)
# plot(cond.rf)
# abline(h=0.000128,col="red",lwd=2)
}
```


Now we are going to go through the process of finding optimal number of trees (ntree) and number of variables to randomly pick at each stage (mtry)

```{r}
#1st: choose optimal mtry's

# results = rf.sscv(cond.rf,X,mtry=3)
# results = rf.sscv(cond.rf,X,mtry=4)
# results = rf.sscv(cond.rf,X,mtry=5)
# results = rf.sscv(cond.rf,X,mtry=6)

{#m=4
# m=4
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
# #m=5
# m=5
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
# #m=6
# m=6
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
# 
# #m=7
# m=7
# myforest = function(formula,data){randomForest(formula,data,mtry=m)}
# error.RF = numeric(10)
# for (i in 1:10) error.RF[i] = errorest(y~.,
#                                        data=X,model=myforest)$error
# mean(error.RF)
# summary(error.RF)
}
#2nd: Increasing the number of trees beyonf the default ntree=500
#BEST = based on best mtry from above
#BEST = 4

# results = rf.sscv(cond.rf,X,mtry=BEST,ntree=500,B=250)
# results = rf.sscv(cond.rf,X,mtry=BEST,ntree=750,B=250)
# results = rf.sscv(cond.rf,X,mtry=BEST,ntree=1000,B=250)

#mtry=4,ntree=1000

```



```{r}
###### Compare best mtry and ntree combos #######                        
#ntree of 50, 80, 100, 350

# cond.rf = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,ntree=40)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,ntree=80)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,ntree=750)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,ntree=350)
# results = rf.sscv(cond.rf,X.train)
# cond.rf = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,ntree=150, mtry = 6)
# results = rf.sscv(cond.rf,X.train)

# #mtry

# cond.rf = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,mtry = 4,ntree=1000)
# results = rf.sscv(cond.rf,X.train)
# cond.final = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,mtry=5,ntree=750)
# results = rf.sscv(cond.rf,X.train)

#mtry=4, ntree=1000
#mtry=5, ntree=750

#Best combo is: mtry=4 and ntree=1000

```

Our thorough process of exploring different number of bootstrap trees and number of variables to randomly pick at each stage gave us the following optimal hyperparameters: Best combo is: <b> mtry=4 and ntree=1000 </b>

This gives us our final model:

```{r}
####### Final randomForest model #######
{cond.final = randomForest(bandgap_energy_ev_log_y_plus_1~.,data=X.train,mtry=4,ntree=1000, importance = T)
ypred = predict(cond.final,newdata=X.valid)
prediction = PredAcc(y.valid, ypred)
}
```

Let's see how our final random forest model predicts. We are plotting the actual formation energy values vs. predicted for both the training set data (figure on the left) and validaiton set data (figure on the right)

```{r fig.height=5, fig.width=10}
{par(mfrow=c(1,2))
plot(y.train,predict(cond.final),xlab="Actual Formation Energy",ylab="Fitted Formation Energy",
     main= paste("Training Data", " R^2: ", 
                 round((cor(y.train, 
                            predict(cond.final))^2),4)))
abline(0,1,col="red",lwd=2)

plot(y.valid, ypred, xlab="Actual Formation Energy",ylab="Predicted Formation Energy",
     main=paste("Test Data", "R^2: ",
                round((cor(y.valid, 
                           ypred)^2),4)))
abline(0,1,col="red",lwd=2)
#par(mfrow=c(1,1))
}
```

Our final random forest model predicts with accuracy of RMSLEP=0.05864 and gives us 91.27% rsquared value. It is not a good model to predict bandgap energy. Lasso, Elastic Net (alpha=0.5), Treed Regression, MARS, and XGboosting are performing better than Random Forest for Model 2.

Our last statistical model to consider are Neural Networks

## Neural Networks

```{r include=FALSE}
###### Libraries #####
{
  library(tidyr)
  library(dplyr)
  library(ggplot2)
  library(nnet)
  #library(Ecfun)
  library(car)
  library(ISLR)
  #library(MASS)
  library(glmnet)
  library(pls)
  library(corrplot)
  require(stringr)
  library(gridExtra)
  library(ipred)
  library(randomForest)
  library(nnet)
  library(GGally)
}
```

Functions:
```{r}
###### Functions #####
#detach("package:Ecfun", unload = TRUE)
#rf.sscv Monte-Carlo CV function
{rf.sscv = function(fit,data,p=.667,B=100,mtry=fit$mtry,ntree=fit$ntree) {
  RMSEP = rep(0,B)
  RMSLEP = rep(0,B)
  MAEP = rep(0,B)
  MAPEP = rep(0,B)
  y = fit$y
  n = nrow(data)
  ss <- floor(n*p)
  for (i in 1:B) {
    sam = sample(1:n,ss,replace=F)
    fit2 = randomForest(formula(fit),data=data[sam,],mtry=mtry,ntree=ntree)
    ynew = predict(fit2,newdata=data[-sam,])
    RMSEP[i] = sqrt(mean((y[-sam]-ynew)^2))
    RMSLEP[i] = sqrt(mean((log(ynew +1) - log(y[-sam] +1))^2))
    MAEP[i] = mean(abs(y[-sam]-ynew))
    MAPEP[i] = mean((abs(y[-sam]-ynew)/y[-sam]))
    
  }
  RMSEP = mean(RMSEP)
  RMSLEP = mean(RMSLEP)
  MAEP = mean(MAEP)
  MAPEP = mean(MAPEP)
  cat("RMSEP\n")
  cat("===============\n")
  cat(RMSEP,"\n\n")
  cat("RMSLEP\n")
  cat("===============\n")
  cat(RMSLEP,"\n\n")
  cat("MAEP\n")
  cat("===============\n")
  cat(MAEP,"\n\n")
  cat("MAPEP\n")
  cat("===============\n")
  cat(MAPEP,"\n\n")
  temp = data.frame(RMSEP=RMSEP, RMSLEP=RMSLEP, MAEP=MAEP,MAPEP=MAPEP)
  return(temp)
}

}
#Predict Accuracy function
{PredAcc = function(y, ypred){
  RMSEP = sqrt(mean((y-ypred)^2))
  RMSLEP = sqrt(mean((log(ypred +1) - log(y+1))^2))
  MAE = mean(abs(y-ypred))
  MAPE = mean(abs(y-ypred)/y)*100
  cat("RMSEP\n")
  cat("================\n")
  cat(RMSEP, "\n\n")
  cat("RMSLEP\n")
  cat("================\n")
  cat(RMSLEP, "\n\n")
  cat("MAE\n")
  cat("================\n")
  cat(MAE, "\n\n")
  cat("MAPE\n")
  cat("================\n")
  cat(MAPE, "\n\n")
  return(data.frame(RMSEP = RMSEP, RMSLEP = RMSLEP, MAE = MAE, MAPE = MAPE))
}
}
#Plotting variable importance
{rfimp = function(rffit, horiz=T) {barplot(sort(rffit$importance[,1]),horiz=horiz,
                                           xlab="Mean Decreasing in Accuracy",main="Variable Importance")
}
}

```

The data was loaded and prepared in the Shirnkage Methods section. We are going to continue working with train_model1 and train_model2 data frames.
```{r echo=TRUE, include=FALSE}
###### Model 1 #####
{y = train_model1$formation_energy_ev_natom_log_y_plus_1
X = train_model1
}

#tcX = train_model1_no_box_cox[,c(1,2,25,26,27)]
#ggpairs(tcX)
#require(caret)
set.seed(1111)
mod1 = nnet(formation_energy_ev_natom_log_y_plus_1~., data = X, size=6, skip=T, linout=T, decay=0.0001, maxit=25000)
#summary(mod1)
```


```{r fig.height=4, fig.width=5}
plot(X$formation_energy_ev_natom_log_y_plus_1, fitted(mod1))
abline(0,1, lwd=2, col="blue")
```

We are now splitting the training set into the training and validation sets

```{r}
###### Create train, dev, and test sets #######

{
n = nrow(X)
set.seed(1111)
sam = sample(1:n,size=floor(n*.6667),replace=F)
X.train = X[sam,] #form training dataset
X.valid = X[-sam,] #form validation dataset
y.train = X.train$formation_energy_ev_natom_log_y_plus_1
y.valid = X.valid$formation_energy_ev_natom_log_y_plus_1
}
```

Fit three different Neural Net models with different hyperparameters:

```{r echo=TRUE, include=FALSE}
nn1 = nnet(formation_energy_ev_natom_log_y_plus_1~., data = X.train, size=6, skip=F, linout=T, decay=0.01, maxit=25000)
nn2 = nnet(formation_energy_ev_natom_log_y_plus_1~., data = X.train, size=8, skip=F, linout=T, decay=0.005, maxit=25000)
nn3 = nnet(formation_energy_ev_natom_log_y_plus_1~., data = X.train, size=8, skip=T, linout=T, decay=0.05, maxit=25000)

```

Assign the Neural Net model into 'nn' below to check how it predicts and fits our data:

```{r fig.height=5, fig.width=10}
#best nn2 so far
nn = nn2

{ypred = predict(nn, newdata = X.valid)
prediction = PredAcc(y.valid, ypred)
}
```

```{r fig.height=5, fig.width=10}
{par(mfrow=c(1,2))
plot(y.train,predict(nn),xlab="Actual Formation Energy",ylab="Fitted Formation Energy",
     main= paste("Training Data", " R^2: ", 
                 round((cor(y.train, 
                            predict(nn1))^2),4)))
abline(0,1,col="red",lwd=2)

plot(y.valid, ypred, xlab="Actual Formation Energy",ylab="Predicted Formation Energy",
     main=paste("Test Data", "R^2: ",
                round((cor(y.valid, 
                           ypred)^2),4)))
abline(0,1,col="red",lwd=2)
par(mfrow=c(1,1))
}
```

Our final Neural Network model predicts with accuracy of RMSLEP=0.03075 and gives us 82.39% rsquared value. It is not a good model to predict formaiton energy. 

###Model 2

```{r echo=TRUE, include=FALSE}
###### Model 2 #####
{y = train_model2$bandgap_energy_ev_log_y_plus_1
X = train_model2
}

#tcX = train_model1_no_box_cox[,c(1,2,25,26,27)]
#ggpairs(tcX)
#require(caret)
set.seed(1111)
mod1 = nnet(bandgap_energy_ev_log_y_plus_1~., data = X, size=6, skip=T, linout=T, decay=0.0001, maxit=25000)
#summary(mod1)
```


```{r fig.height=4, fig.width=5}
plot(X$bandgap_energy_ev_log_y_plus_1, fitted(mod1))
abline(0,1, lwd=2, col="blue")
```

We are now splitting the training set into the training and validation sets

```{r}
###### Create train, dev, and test sets #######

{n = nrow(X)
set.seed(1111)
sam = sample(1:n,size=floor(n*.6667),replace=F)
X.train = X[sam,] #form training dataset
X.valid = X[-sam,] #form validation dataset
y.train = X.train$bandgap_energy_ev_log_y_plus_1
y.valid = X.valid$bandgap_energy_ev_log_y_plus_1

#cond.rf = randomForest(y.train~.,data=X.train)
#plot(cond.rf)
#abline(h=0.000128,col="red",lwd=2)
} 
```

Fit three different Neural Net models with different hyperparameters:
```{r echo=TRUE, include=FALSE}
nn1 = nnet(bandgap_energy_ev_log_y_plus_1~., data = X.train, size=6, skip=F, linout=T, decay=0.01, maxit=25000)
nn2 = nnet(bandgap_energy_ev_log_y_plus_1~., data = X.train, size=8, skip=F, linout=T, decay=0.005, maxit=25000)
nn3 = nnet(bandgap_energy_ev_log_y_plus_1~., data = X.train, size=10, skip=T, linout=T, decay=0.05, maxit=25000)

```

Assign the Neural Net model into 'nn' below to check how it predicts and fits our data:

```{r}
#best nn3 so far
nn = nn3

{ypred = predict(nn, newdata = X.valid)
prediction = PredAcc(y.valid, ypred)
}
```

```{r fig.height=5, fig.width=10}
{ 
  par(mfrow=c(1,2))
  plot(y.train,predict(nn1),xlab="Actual Formation Energy",ylab="Fitted Formation Energy",
       main= paste("Training Data", " R^2: ", 
                   round((cor(y.train, 
                              predict(nn1))^2),4)))
  abline(0,1,col="red",lwd=2)
  
  plot(y.valid, ypred, xlab="Actual Formation Energy",ylab="Predicted Formation Energy",
       main=paste("Test Data", "R^2: ",
                  round((cor(y.valid, 
                             ypred)^2),4)))
  abline(0,1,col="red",lwd=2)
  par(mfrow=c(1,1))
}

```

Our final Neural Network model predicts with accuracy of RMSLEP=0.0618 and gives us 91.42% rsquared value. It is not a good model to predict bandgap energy. 

## Keras Regressor: dense neural network
```{r}
####### KERAS ########
require(keras)
{y = train_model1$formation_energy_ev_natom_log_y_plus_1
train_data = as.matrix(scale(train_model1))
}
```

```{r}
build_model <- function() {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
                input_shape = dim(train_data)[2]) %>%
    layer_dropout(0.6) %>%
    layer_dense(units = 64, activation = "relu") %>%
    layer_dropout(0.6) %>%
    layer_dense(units = 1, activation = "linear")
  
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_rmsprop(),
    metrics = list("mean_squared_logarithmic_error")
  )
  
 return(model)
}
```

```{r}
model <- build_model()
model %>% summary()
```

```{r}
# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)    
epochs <- 50
```

```{r}
# Fit the model and store training stats
history <- model %>% fit(
  train_data,
  y,
  epochs = epochs,
  validation_split = 0.2,
  verbose = 0,
  callbacks = list(print_dot_callback)
)

```

```{r}
sqrt(mean(history$metrics$mean_squared_logarithmic_error))
```

Not performing well...

#Final Models

Our Random Forest model performed the best in prdicting the formation of energy, whereas the XGboost Model performed the best in predicting the bandgap energy

Below, is the comparison of all the models we ran, in predicting formation energy and bandgap energy:
```{r}
#Sorted for RMSLEP for formation energy
models.all = read.csv("model_comparison.csv")
models.all
```

```{r}
#Sorted for RMSLEP for bandgap energy
models.all[order(models.all$RMSLEP.for.bandgap),]
```


## Predicting Formation of Energy: Random Forest

Revisiting the best model and going over the variable importance:

```{r}
#randomforest.final
```












































































































































































































































































































## Predicting Bandgap Energy: XGboost


```{r}
# cond_test_ready %>%
#   select (-c(bandgap_energy_ev_log_y_plus_1,formation_energy_ev_natom_log_y_plus_1,ID)) -> Modified_Xs
# 
# Bandgap_Predictions = predict(XG_band, as.matrix(Modified_Xs))
# 
# write.csv(Bandgap_Predictions, file = "bandgap_predictions_XGboost.csv")

```

As a reminder, using this model:

```{r}
# XG_band = xgboost(data = form_test_x2_mat, label = form_test_y2_list, nrounds = 210, nthread = 7, verbose = T, objective = "reg:squaredlogerror", eta = 0.4, max.depth = 1, lambda = 0.5, alpha = 0, stratified = F, gamma = 0, colsample_bytree=1, maximize = 0, min_child_weight=15)
```


For predicting bandgap energy using RMSLE as our metric, our XGBoost won out over every other model we tried (0.029176). Digging deeper into it, we found that the algorithm performed best going around 210 rounds with a maximum of 7 threads for any given boost. A ridge regression component did glean some improvement, with oging about halfway to mximium potential being implemented. Oddly, LASSO components were found to only detract from predictive power, and were thus not included. Each child node needed to have at least 15 instances of it being used to be created, and the most layers any single tree could have is 6. The number of variable supplied to the trees was maximized. The learning rate was fairly low at 0.4, which seems low, but makes more sense when one looks at what the structure of the top trees looks like.



Variable importance for final model for bandgap energy using top 10 predictors.


```{r}
mat = xgb.importance(feature_names=colnames(form_test_x2_mat),model=XG_band)
xgb.plot.importance(importance_matrix=mat[1:10])

```

Looking at all predictors

```{r}
mat = xgb.importance(feature_names=colnames(form_test_x2_mat),model=XG_band)
xgb.plot.importance(importance_matrix=mat[1:26])
```

We can see from the charts above that the most important varaibles by far were what percent of Indium and Aluminum were in each conductor, with the two of the being repsonsible for almost 70% of the result. Additionally, only a small portion of our variable were actually used, with both existing metrics and new ones we created being used. Spacegroups themselves numerically were veary important, as was the category specfically calling out spacegroup 167, which in itself was distinct enough to warrent a little over 2% importance. Lattice vector angles 1 and 3, and angle degrees gamme and alpha were also used, with and beta and vector 2 being absent, likely as geometrically the information can be considered redudant. Beyond that, nost variables were ignored, probably due to either again being redundant or simply not reelvant.


Looking at the all the tree plots is not really practical with XGboost, especially considering we would have over 200 0f them due to how the trees are structered. Below is shown the first few trees in the boosted chain just to give an idea as to what each boost does

```{r}
require(DiagrammeR)
xgb.plot.tree(model = XG_band, trees = 0, show_node_id = T)

```

```{r}
xgb.plot.tree(model = XG_band, trees = 1, show_node_id = T)
```

```{r}
xgb.plot.tree(model = XG_band, trees = 2, show_node_id = T)
```

```{r}
xgb.plot.tree(model = XG_band, trees = 3, show_node_id = T)
```


Note how the first couple of trees with the largest impact are all very simple. Given the small amount of variables and our decision to lower the learning rate along with having a few key predictors found to be useful, it makes sense that our extreme boosting mostly covered simple trees that by themselves were still fairly compentant, which lines up with the results from our MARS and Shrinkage Models, which tended to follow similar patterns in terms of variable importance.

XGboosts implementation does not a good way to tell specifically how the variables effect the data. However, we can use our other models that performed almost as well to get at least a similar understanding as to how specific predictors affect the results. Our MARS model, being the second best for our result, only being about 0.02 RMSLE off, should work nicely.

```{r}

cond_train3_band_pifs = read.csv("cond_train3_band_stup.csv")

band.mars2 = earth(bandgap_energy_ev_log_y_plus_1~.,degree = 2, data = cond_train3_band_pifs, nk = 30, nfold = 20, nprune =  15, pmethod = "exhaustive" )

plotmo(band.mars2, caption = NULL )
```

Variable names had to be changed in order to prevent plotmo from failing to provide readable plots, as shown in the new read in.

From these plots generated using our model, we can tell that if one wanted to maximize bandgap energy they should add a higher percentage of Aluminum atoms,  have a combination of less vector degree 3 and less lattice angle alpha, and reduce the total amount of indium present in the formulation, and crank up lattice angle gamma,. However, if one is to increase angle gamma, also add in a little bit of vector angle 1 to get the maximal return. For spacegroup, go with specifically spacegroup 167 if you can, as it is really the only spacegroup to meanifully increase this response.







