---
title: "Conductor_Project"
author: "Samuel A, Mikolaj W"
date: "April 3, 2020"
output: word_document
---


Preperation Work and packages
```{r include=FALSE}

setwd("C:/Users/qs5834mm/Desktop/DSCI 425/Git_R/Assignment_2/MLR-ACE-AVAS-MARS/Conductor Formulations")
load("C:/Users/qs5834mm/Desktop/DSCI 425/R/mult.Rdata")
load("C:/Users/qs5834mm/Desktop/DSCI 425/R/Regression.Rdata")

cond_com =  read.csv("Conductors (combined).csv")
cond_train = read.csv("Conductors (train).csv")
cond_teat = read.csv("Conductors (test).csv")

require(dplyr)
require(ggplot2)
require(tidyr)
require(stringr)
library(car)
library(Ecfun)



View(cond_com)



```

Format the response in the training set to ln(y + 1).
```{r}
cond_train2 = cond_train

cond_train %>%
  mutate(formation_energy_ev_natom_log_y_plus_1 = log(formation_energy_ev_natom + 1), 
         bandgap_energy_ev_log_y_plus_1 = log(bandgap_energy_ev + 1)) -> cond_train2

drop.cols <- c('formation_energy_ev_natom', 'bandgap_energy_ev')

cond_train2 %>%
  select(-all_of(drop.cols)) -> cond_train2
  

```

Starting out, well try and see what transformations would be optimal for existing variables. There are only 11 in total be defualt, so going through each on an individual basis is more then doable. We also will follow a doctrine of optimization being greater then explainability for this approach, as one of our chief goals is predictions quality of predictions.


```{r}

cond_train2_trans = cond_train2

#checks for optimal transformations
myBC = function(y)
{
  BCtran(y)
  results = powerTransform(y)
  summary(results)
}


```

Space group itself is an interesting variable, as while it seems ordinal at first, space groups themselves are a bit complex and a higher group number is not a consitant difference. This will be accounted for later.

Starting with Total Atoms
```{r}

Statplot(cond_train2$number_of_total_atoms)
myBC(cond_train2$number_of_total_atoms)

cond_train2_trans$number_of_total_atoms = bcPower(cond_train2$number_of_total_atoms, 1.6)

Statplot(cond_train2_trans$number_of_total_atoms)

```


These next three variables measure the makeup of the elements Galluim, Aluminum, and Indium as percentages. All three are of course related in that regard, and certain makeups may include some
or all of each of these. This may make it difficult to view each as an individual in terms of model
utilization, but there are a few things we can try. First, we can go through and simply transform each
variable.


Percent_atom_al is next
```{r}

Statplot(cond_train2$percent_atom_al)



```
Since we are workiing with percentage data, we thought it would be best to use a logit trasnformation here.

```{r}
cond_train2_trans$percent_atom_al = logit(cond_train2$percent_atom_al)

Statplot(cond_train2_trans$percent_atom_al)
```
The improvement is noticable, although the zeros still tend to have a negative impact.

Next was percent_atom_ga

```{r}
Statplot(cond_train2$percent_atom_ga)
```
This variale similarly was best handled with logit.

```{r}
cond_train2_trans$percent_atom_ga = logit(cond_train2$percent_atom_ga)

Statplot(cond_train2_trans$percent_atom_ga)
```

We still have a problem with zeros, although in every other regard we have improved.


percent_atom_in is the final variable in this series, and likewise was handled similarly.

```{r}
Statplot(cond_train2$percent_atom_in)
```


```{r}
cond_train2_trans$percent_atom_in = logit(cond_train2$percent_atom_in)

Statplot(cond_train2_trans$percent_atom_in)
```



Next, we have a series of lattice vector variables that should be simple enough to box cox transform.

```{r}
Statplot(cond_train2$lattice_vector_1_ang)
```

Our myBC function tends to indicate using a power of 0.6, however, better results through testing were found in the for of a 4th root transformation. While the result is still not ideal, the improvement is still massive and should not be discounted. This may prove to be the begining of a pattern in terms of how the lattive vectors all hold.

```{r}
myBC(cond_train2$lattice_vector_1_ang)

cond_train2_trans$lattice_vector_1_ang = bcPower(cond_train2$lattice_vector_1_ang, 0.25)

Statplot(cond_train2_trans$lattice_vector_1_ang)
```
The next vector angle has similar shortcomings, but uses a different solution.

```{r}
Statplot(cond_train2$lattice_vector_2_ang)
```

This time, the bcPower function was more on target, with the power transformation of 0.7 being the best. Unfotunatley, the best in this case is not a very desireable variable structure, but once again still is an improvement.
```{r}
myBC(cond_train2$lattice_vector_2_ang)

cond_train2_trans$lattice_vector_2_ang = bcPower(cond_train2$lattice_vector_2_ang, 0.7)

Statplot(cond_train2_trans$lattice_vector_2_ang)
```
For the last vector angle, we have a similar story.

```{r}
Statplot(cond_train2$lattice_vector_3_ang)
```
The box cox gave the best result again with a negative 5th root, although much like the former our results still have shortcomings despite the imrpovements.
```{r}
myBC(cond_train2$lattice_vector_3_ang)

cond_train2_trans$lattice_vector_3_ang = bcPower(cond_train2$lattice_vector_3_ang, -0.25)

Statplot(cond_train2_trans$lattice_vector_3_ang)
```
The next three predictors are angle degrees, which all seem to hover around 90 degrees with occasional variation coming off of each. This will hopefull scale easily as a result.

Starting with lattice angle alpha degree

```{r}
Statplot(cond_train2$lattice_angle_alpha_degree)
```

We can see some unique problems due to the nature of degree mseasures here, so we will try converting to radians in order to assist in this.

```{r}
radians_to_degree <- function(rad) {
  (rad * 180) / (pi)
}

degree_to_rad <- function(deg) {
  (deg * pi) / (180)
}


cond_train2_trans$lattice_angle_alpha_degree = degree_to_rad(cond_train2$lattice_angle_alpha_degree)

Statplot(cond_train2_trans$lattice_angle_alpha_degree)
```
This should hopefully allow for easier transformation.



```{r}
myBC(cond_train2_trans$lattice_angle_alpha_degree)

cond_train2_trans$lattice_angle_alpha_degree = bcPower(cond_train2_trans$lattice_angle_alpha_degree, -11.5 )

Statplot(cond_train2_trans$lattice_angle_alpha_degree)
```

However, while it allows us to at least transform the data, it still does not allow us to truly fix it. We can box cox transform it with an abhorrent -11.5, but the massive amount of observations that close in around 90 simply leave us with odd results. 

We see similar issues with the next to angle variables

```{r}
cond_train2_trans$lattice_angle_beta_degree = degree_to_rad(cond_train2$lattice_angle_beta_degree)

Statplot(cond_train2_trans$lattice_angle_beta_degree)
```

```{r}
myBC(cond_train2_trans$lattice_angle_beta_degree)

cond_train2_trans$lattice_angle_beta_degree = bcPower(cond_train2_trans$lattice_angle_beta_degree, -15.5 )

Statplot(cond_train2_trans$lattice_angle_beta_degree)
```

```{r}

cond_train2_trans$lattice_angle_gamma_degree = degree_to_rad(cond_train2$lattice_angle_gamma_degree)

Statplot(cond_train2_trans$lattice_angle_gamma_degree)
```

```{r}
myBC(cond_train2_trans$lattice_angle_gamma_degree)

cond_train2_trans$lattice_angle_gamma_degree = bcPower(cond_train2_trans$lattice_angle_gamma_degree, 2.2)

Statplot(cond_train2_trans$lattice_angle_gamma_degree)
```

Ultimatley, what this process shows us is that in their current states, even transformed, many of these variables are not likely to be of great use to us. However, with a little enginuity we might be able to stretch these a little farther.


Going back to Space groups, we can create categorical variables for each group as a way to focus in on that variables, as only 6 of the possible 230 existing space groups are actually used.


```{r}
require(sjmisc)

cond_train2 %>% 
  to_dummy(spacegroup, suffix = "label" ) %>% 
  bind_cols(cond_train2) %>% 
  select(ID, spacegroup, everything()) -> cond_train3

cond_train3 %>%
  rename(Spacegroup_12 = V1, Spacegroup_33 = V2, Spacegroup_167 = V3, Spacegroup_194 = V4, Spacegroup_206 = V5, Spacegroup_227 = V6) -> cond_train3


```



Number of total atoms, while numeric, only has 6 unique values across 1,567 rows. While the consistancy is nice, it does make our ability to transform it into a normal scale fairly difficuly as seen pervisouly. To circumvent this problem, we have elected to turn it into categorical data as well, as we feel that the benefit of being able to focus in on a category of atoms outweights the loss in ordinal power. However, certain models still will be fed one or the other based on their preference.

```{r}
cond_train3 %>% 
  to_dummy(number_of_total_atoms, suffix = "label" ) %>% 
  bind_cols(cond_train3) %>% 
  select(ID, number_of_total_atoms, everything()) -> cond_train3

  cond_train3 %>%
  rename(  atoms_10 = V1, atoms_20 = V2, atoms_30 = V3, atoms_40 = V4, atoms_60 = V5, atoms_80 = V6) -> cond_train3



```


We also want to make dummy varaibles based on whether or not certain elements are present within a potential superconductor, as this can help mitigate colineation problems in situations where these percentage varaibles would otherwise be related, and additionally they may prove useful on their own.


```{r}
cond_train3 %>%
  mutate(Aluminum = ifelse(percent_atom_al > 0, 1, 0), Gallium = ifelse(percent_atom_ga > 0, 1, 0),  Indium = ifelse(percent_atom_in > 0, 1, 0)) -> cond_train3


```


```{r}

#write.csv(cond_train3, "cond_train_new_variables.csv")

```





